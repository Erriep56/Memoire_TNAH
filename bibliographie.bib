@unpublished{adamovaProjetGallicaImages2024,
  title = {Le projet {{Gallica Images}}. {{L}}’intelligence artificielle pour l’accès aux grands corpus d’images},
  author = {Adamova, Alexandra and Wingert, Rosanne},
  date = {2024-06-18},
  url = {https://www.youtube.com/watch?v=IVMfpR96jrk},
  urldate = {2025-09-04},
  abstract = {Présentation de Alexandra Adamova (BnF) et Rosanne Wingert (BNU). Rencontre régionale Grand Est « Intelligence artificielle, patrimoine et humanités numériques. Organisé par le chapitre francophone AI4LAM, La Bibliothèque nationale et universitaire de Strasbourg, L’Ecole nationale des chartes, Le projet ANR CiSaMe et la MISHA (Maison Interuniversitaire des Sciences de l’Homme – Alsace) de Strasbourg. Créé en 2022, le chapitre francophone de la communauté AI4LAM a vocation à rassembler une communauté de pratique en langue française autour de l’application des technologies d’intelligence artificielle dans les bibliothèques, archives et musées. L’émergence de plusieurs projets dans le Grand Est, au croisement entre les institutions patrimoniales et le monde universitaire, entre pratiques professionnelles et humanités numériques, a créé un terrain favorable pour l’organisation d’une rencontre en présentiel, la première depuis la réunion de préfiguration du chapitre à la BnF en 2021. La Bibliothèque nationale et universitaire de Strasbourg avec son Lab sera l’hôte de cette journée, qui se déroulera en deux temps : le matin, une plénière en auditorium au cours de laquelle plusieurs projets et perspectives seront présentés, et l’après-midi, des ateliers en petits groupes pour échanger idées, besoins et expériences diverses autour de thématiques choisies par les participants.}
}

@online{albouyAIKONComputerVision2024,
  title = {{{AIKON}}: a computer vision platform for historians},
  author = {Albouy, Ségolène and Norindr, Jade and Aouinti, Fouad and Grometto, Clara and Champenois, Robin and Guilbaud, Alexandre and Lazaris, Stavros and Husson, Matthieu and Aubry, Mathieu},
  date = {2024},
  url = {https://aikon-platform.github.io/},
  urldate = {2025-09-04}
}

@book{amarFondementsTheoriquesLindexation2000,
  title = {Les fondements théoriques de l'indexation, une approche linguistique},
  author = {Amar, Muriel},
  date = {2000},
  publisher = {ADBS Éditions},
  location = {Paris}
}

@online{aspertiDoesCLIPPerceive2025,
  title = {Does {{CLIP}} perceive art the same way we do?},
  author = {Asperti, Andrea and Dessì, Leonardo and Tonetti, Maria Chiara and Wu, Nico},
  date = {2025},
  doi = {10.48550/ARXIV.2505.05229},
  url = {https://arxiv.org/abs/2505.05229},
  urldate = {2025-08-30},
  abstract = {CLIP has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it "see" the same way humans do - especially when interpreting artworks? In this paper, we investigate CLIP's ability to extract high-level semantic and stylistic information from paintings, including both human-created and AI-generated imagery. We evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. By designing targeted probing tasks and comparing CLIP's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. Our findings reveal both strengths and limitations in CLIP's visual representations, particularly in relation to aesthetic cues and artistic intent. We further discuss the implications of these insights for using CLIP as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. Our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.},
  pubstate = {prepublished},
  version = {1},
  keywords = {68T45 68T07 (Primary) 68T50 68U10 (Secondary),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,I.2.7; I.2.10,Multimedia (cs.MM)}
}

@book{associationfrancaisedenormalisationDocumentationRecueilNormes1993,
  title = {Documentation: recueil de normes françaises 1993},
  shorttitle = {Documentation},
  editor = {{Association française de normalisation}},
  date = {1993},
  edition = {5e éd},
  publisher = {AFNOR},
  location = {Paris-La Défense},
  isbn = {978-2-12-234450-7 978-2-12-234451-4 978-2-12-234452-1},
  langid = {fre}
}

@article{aubryArtificialIntelligenceArt2021,
  title = {Artificial Intelligence and Art History: A Necessary Debate?},
  shorttitle = {Artificial Intelligence and Art History},
  author = {Aubry, Mathieu and Costiner, Lisandra and James, Stuart},
  date = {2021},
  journaltitle = {Histoire de l'art},
  shortjournal = {hista},
  volume = {87},
  number = {1},
  pages = {55--60},
  issn = {0992-2059},
  doi = {10.3406/hista.2021.3936},
  url = {http://www.persee.fr/doc/hista_0992-2059_2021_num_87_1_3936},
  urldate = {2025-08-01},
  abstract = {Les réflexions sur l’intelligence artificielle et son impact sur la société semblent omniprésentes. Quelles en sont les implications pour l’histoire de l’art ? Alors que le mouvement de numérisation est désormais bien avancé et qu’il a déjà des effets importants en termes d’accessibilité, des outils plus affinés sont encore en développement. L’impact à long terme de ces nouvelles techniques demeure cependant largement inconnu pour le moment. Deux jeunes chercheurs en humanités numériques venant d’horizons différents, à savoir Lisandra Costiner, formée en histoire de l’art, et Stuart James, formé en informatique, échangent dans ce débat leurs opinions sur l’impact de l’intelligence artificielle sur l’histoire de l’art.           ,              Reflections on artificial intelligence and its impact on society seem omnipresent. What does it entail for art history ? While digitization is already well under way and has had an important impact in terms of accessibility, more advanced tools are still under development. In both cases, the long-term implications of these new techniques are largely unknown. We asked two young digital humanities researchers from different backgrounds, namely Lisandra Costiner, trained in art history, and Stuart James, trained in computer science, for their opinions regarding the impact of artificial intelligence on art history.},
  langid = {french}
}

@online{AugmentedArtworkAnalysis,
  title = {Augmented Artwork Analysis (AAA)},
  shorttitle = {https},
  url = {https://icar.cnrs.fr/aaa/},
  urldate = {2025-08-25},
  langid = {french}
}

@mvbook{barocchiFirstInternationalConference1978,
  title = {First {{International Conference}} on {{Automatic Processing}} of {{Art History Data}} and {{Documents}}, {{Pisa}}, {{Scuola Normale Superiore}}, 4-7 {{September}} 1978 : conference transactions},
  author = {Barocchi, Paola and Bisogni, Fabio and Corti, Laura and {al.}},
  date = {1978},
  publisher = {[s.n.]},
  location = {[s.l.]},
  volumes = {2}
}

@book{barralialtetDictionnaireCritiqueDiconographie2003,
  title = {Dictionnaire critique d'iconographie occidentale},
  author = {Barral i Altet, Xavier and {al.}},
  date = {2003},
  publisher = {Presses Universitaires de Rennes},
  location = {Rennes}
}

@incollection{barralialtetIntroduction2003,
  title = {Introduction},
  booktitle = {Dictionnaire critique d'iconographie occidentale},
  author = {Barral i Altet, Xavier},
  date = {2003},
  pages = {866},
  publisher = {Presses Universitaires de Rennes},
  location = {Rennes}
}

@online{beaudOntologieChristianismeMedieval2023,
  title = {Ontologie du christianisme médiéval en images},
  author = {Beaud, Mathieu and Marchesin, Isabelle and Biay, Sébastien},
  date = {2023},
  url = {https://omci.inha.fr/},
  urldate = {2025-08-27}
}

@mvbook{bergeonlanglePeintureDessinVocabulaire2009,
  title = {Peinture \& dessin: vocabulaire typologique et technique},
  shorttitle = {Peinture \& dessin},
  author = {Bergeon Langle, Ségolène and Curie, Pierre},
  date = {2009},
  series = {Principes d'analyse scientifique},
  publisher = {Éditions du patrimoine : Centre des monuments nationaux},
  location = {Paris},
  isbn = {978-2-7577-0065-5},
  volumes = {2},
  keywords = {Artists' materials,Dictionaries,Drawing,Painting,Technique,Terminology}
}

@inproceedings{bermesRepenserCollectionsPatrimoniales2025,
  title = {Repenser les collections patrimoniales par le prisme de l'{{IA}} 2025},
  booktitle = {Conférence nationale sur les applications de l'{{Intelligence}} artificielle},
  author = {Bermès, Emmanuelle and Charpier, Marion},
  date = {2025},
  location = {Dijon, France},
  url = {https://hal.science/hal-05138697},
  keywords = {Collections patrimoniales,Computer vision,Coop\'eration humain-machine,Heritage collections,Humanmachine cooperation,Mus\'ee,Museum,Vision par ordinateur}
}

@online{bibliothequenationaledefranceMandragore,
  title = {Mandragore},
  author = {{Bibliothèque nationale de France}},
  url = {https://mandragore.bnf.fr/},
  urldate = {2025-09-03}
}

@online{bibliothequenationaledefranceProjetsIntelligenceArtificielle,
  title = {Les projets en intelligence artificielle à la BnF},
  author = {{Bibliothèque nationale de France}},
  url = {https://www.bnf.fr/fr/les-projets-en-intelligence-artificielle-la-bnf},
  urldate = {2025-09-01},
  abstract = {BnF - Site institutionnel},
  langid = {french},
  organization = {BnF - Site institutionnel}
}

@online{bibliothequenationaledefranceThesaurusIconographiqueMandragore2024,
  title = {Thésaurus iconographique de Mandragore},
  author = {{Bibliothèque nationale de France}},
  date = {2024},
  url = {https://api.bnf.fr/fr/thesaurus-mandragore},
  urldate = {2025-08-24},
  langid = {french}
}

@inproceedings{boutePANOPTICOutilDexploration2024,
  title = {{{PANOPTIC}}, un outil d’exploration par similarité de vastes corpus d’images},
  booktitle = {Humanistica 2024, {{Association}} francophone des humanités numériques},
  author = {Bouté, Édouard and Julliard, Virginie and Gödicke, David and Pailler, Fred and Ecrement, Victor},
  date = {2024-05},
  location = {Meknès, Maroc (7-9 mai 2024)},
  url = {hal-04687627}
}

@incollection{brandhorstICONCLASSKeyCollaboration2017,
  title = {{{ICONCLASS}}, {{A}} key to collaboration in the digital humanities},
  booktitle = {The {{Routledge Companion}} to {{Medieval Iconography}}},
  author = {Brandhorst, Hans and Posthumus, Etienne},
  editor = {Hourihane, Colum},
  date = {2017},
  edition = {Routledge},
  pages = {201--218},
  location = {Londres et New York}
}

@online{brandhorstICONCLASSplus,
  title = {ICONCLASSplus},
  author = {Brandhorst, Hans},
  url = {https://iconclass.org/help/plus},
  urldate = {2025-09-04},
  abstract = {1 Why register? How to register? Main window layout Browse Search A.I. Search (members) Sample images Bibliography (members) Online source texts (members) Other systems (members) Documentation pages Iconclassplus Edition Basic help},
  langid = {french}
}

@online{brandhorstSurveyVersionsIconclass,
  title = {Survey of versions of the {{Iconclass}} system},
  author = {Brandhorst, Hans and Posthumus, Etienne},
  url = {https://iconclass.org/help/aboutb},
  urldate = {2025-09-03},
  organization = {ICONCLASS}
}

@online{brandhorstVirtualLibraryIconographic2023,
  title = {A virtual library for iconographic research},
  author = {Brandhorst, Hans},
  date = {2023-04-06T12:41:56+00:00},
  url = {https://forum.iconclass.org/t/a-virtual-library-for-iconographic-research/206},
  urldate = {2025-09-03},
  abstract = {A virtual library for iconographic research Henri van de Waal created Iconclass with a dual purpose: 1) to classify the subject matter of images, and 2) to provide access to the iconographic information in scholarly literature.  More than forty years onward, the Iconclass system has indeed been applied to a rich array of image collections, but it has rarely been used to organize bibliographic information.  This post is to let you know we have plans to revive the original Iconclass Bibliography ...},
  langid = {english},
  organization = {ICONCLASS}
}

@misc{brandhorstWordWorthThousand2019,
  title = {A word is worth a thousand pictures - {{Why}} the use of {{Iconclass}} will make {{Artificial Intelligence}} smarter},
  author = {Brandhorst, Hans},
  date = {2019},
  url = {https://iconclass.org/testset/ICONCLASS_and_AI.pdf},
  urldate = {2025-08-01}
}

@online{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021},
  doi = {10.48550/ARXIV.2104.14294},
  url = {https://arxiv.org/abs/2104.14294},
  urldate = {2025-09-01},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{championDigitalHumanitiesText2016,
  title = {Digital humanities is text heavy, visualization light, and simulation poor},
  author = {Champion, Erik Malcolm},
  date = {2016-11-11},
  journaltitle = {Digital Scholarship in the Humanities},
  shortjournal = {Digital Scholarship Humanities},
  pages = {fqw053},
  issn = {2055-7671, 2055-768X},
  doi = {10.1093/llc/fqw053},
  url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqw053},
  urldate = {2025-08-10},
  langid = {english}
}

@mvbook{comanLhistoireLartLinformatique1988,
  title = {L'histoire de l'art et l'informatique documentaire},
  author = {Coman, Florin},
  date = {1988},
  edition = {Atelier national reproduction des thèses, Université Lille III ; Diffusion, Aux Amateurs de livres},
  location = {Lille, Paris},
  isbn = {978-2-905053-56-5},
  volumes = {2},
  keywords = {Art,History Data processing}
}

@online{commissariatgeneralaudeveloppementdurableConsommationDomestiqueEau2023,
  title = {Consommation domestique en eau potable},
  author = {{Commissariat général au développement durable}},
  date = {2023-01-11},
  url = {https://www.notre-environnement.gouv.fr/themes/societe/le-mode-de-vie-des-menages-ressources/article/consommation-domestique-en-eau-potable},
  urldate = {2025-08-31},
  organization = {notre-environnement}
}

@mvbook{cortiAutomaticProcessingArt1984,
  title = {Automatic processing of art history data and documents : [2d. international conference] {{Pisa}}, {{Scuola}} normale superiore, september 24-27, 1984},
  author = {Corti, Laura and Schmitt, Marilyn and {al.}},
  date = {1984},
  publisher = {Regione Toscana},
  location = {Pise},
  volumes = {3}
}

@online{Crotos,
  title = {Crotos},
  url = {https://zone47.com/crotos/},
  urldate = {2025-08-26},
  langid = {french}
}

@online{deeplTranslateText,
  title = {Translate {{Text}}},
  author = {{Deepl}},
  url = {https://developers.deepl.com/api-reference/translate},
  urldate = {2025-09-02},
  abstract = {API reference for translating text with the DeepL API.},
  langid = {english},
  organization = {DeepL Documentation}
}

@article{delucaDigitalEcosystemMultidisciplinary2024,
  title = {A digital ecosystem for the multidisciplinary study of {{Notre-Dame}} de {{Paris}}},
  author = {De Luca, Livio},
  date = {2024},
  journaltitle = {Journal of Cultural Heritage},
  shortjournal = {Journal of Cultural Heritage},
  volume = {65},
  pages = {206--209},
  issn = {12962074},
  doi = {10.1016/j.culher.2023.09.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S129620742300184X},
  urldate = {2025-08-18},
  langid = {english}
}

@article{delucaIntelligenceHumaineCollective2024,
  entrysubtype = {magazine},
  title = {Intelligence humaine, collective et artificielle dans le chantier scientifique et de restauration de Notre-Dame de Paris},
  author = {De Luca, Livio and Guillem, Anaïs and Réby, Kévin},
  date = {2024},
  journaltitle = {Culture et Recherche},
  number = {147},
  pages = {8--12},
  langid = {french}
}

@misc{delucaLecosystemeNumeriqueNdame2022,
  title = {L’écosystème numérique n-dame pour l’analyse et la mémorisation multi-dimensionnelle du chantier scientifique {{Notre-Dame-de-Paris}}},
  author = {De Luca, Livio and Abergel, Violette and Guillem, Anaïs and Malavergne, Olivier and Manuel, Adeline and Néroulidis, Ariane and Roussel, Roxane and Rousset, Miled and Tournon, Sarah},
  date = {2022},
  organization = {SCAN'22}
}

@article{desaint-oursProjetHikarIAEtude2024,
  entrysubtype = {magazine},
  title = {Le projet HikarIA : étude et mise en valeur du patrimoine photographique par l'intelligence artificielle},
  author = {family=Saint-Ours, given=Édouard, prefix=de, useprefix=true and Kermorvant, Christopher},
  date = {2024},
  journaltitle = {Culture et Recherche},
  number = {147},
  pages = {20--24},
  langid = {french}
}

@online{dhlabyalePixPlot,
  title = {{{PixPlot}}},
  author = {{DHLab (Yale)}},
  url = {https://dhlab.yale.edu/projects/pixplot/},
  urldate = {2025-08-30}
}

@inproceedings{dilenardoVisualPatternsDiscovery2016,
  title = {Visual {{Patterns Discovery}} in {{Large Databases}} of {{Paintings}}},
  booktitle = {Digital {{Humanities}} 2016: {{Conference Abstracts}}},
  author = {family=Lenardo, given=Isabella, prefix=di, useprefix=true and Séguin, Benoît and Kaplan, Frédéric},
  date = {2016},
  pages = {169--172},
  publisher = {Jagiellonian University \& Pedagogical University},
  location = {Cracovie},
  url = {https://dh2016.adho.org/abstracts/348},
  urldate = {2025-08-05}
}

@online{DocumentationFastAI,
  title = {Documentation de {{FastAI}}},
  url = {https://docs.fast.ai/},
  urldate = {2025-08-20},
  abstract = {fastai simplifies training fast and accurate neural nets using modern best practices},
  langid = {english}
}

@online{DocumentationLibrairiePython,
  title = {Documentation de la librairie {{Python RDFLib}}},
  url = {https://rdflib.readthedocs.io/en/stable/},
  urldate = {2025-09-01}
}

@online{duhaimePixPlotDepotGithub2018,
  type = {Github repository},
  title = {{{PixPlot}} ({{Dépôt Github}})},
  author = {Duhaime, Douglas and Leonard, Peter},
  date = {2018/2023},
  url = {https://github.com/pleonard212/pix-plot},
  organization = {Github}
}

@mvbook{fondationinternationalepourlelimcLexiconIconographicumMythologiae1981,
  title = {Lexicon {{Iconographicum Mythologiae Classicae}}},
  author = {{Fondation Internationale pour le LIMC}},
  date = {1981/1999},
  publisher = {Artemis},
  location = {Zürich},
  pagetotal = {1209},
  volumes = {IX}
}

@book{fondationinternationalepourlelimcLexiconIconographicumMythologiae2009,
  title = {Lexicon Iconographicum Mythologiae Classicae (LIMC): Supplementum 2009},
  shorttitle = {Lexicon Iconographicum Mythologiae Classicae (LIMC)},
  editor = {{Fondation Internationale pour le LIMC}},
  date = {2009},
  publisher = {Artemis \& Winkler},
  location = {Düsseldorf},
  isbn = {978-3-538-03520-1},
  langid = {fre eng ger ita}
}

@book{garnierThesaurusIconographiqueSysteme1984,
  title = {Thésaurus iconographique, système descriptif des représentations},
  author = {Garnier, François and {Ministère de la Culture}},
  date = {1984},
  publisher = {Le Léopard d'or},
  location = {Paris}
}

@book{gervereauVoirComprendreAnalyser2020,
  title = {Voir, comprendre, analyser les images:},
  shorttitle = {Voir, comprendre, analyser les images},
  author = {Gervereau, Laurent},
  date = {2020},
  series = {Repères},
  publisher = {La Découverte},
  doi = {10.3917/dec.gerv.2020.01},
  isbn = {978-2-348-06022-9}
}

@online{gettyresearchinstituteAAT2024,
  title = {About the {{AAT}}},
  author = {{Getty Research Institute}},
  year = {révisé le 26 juillet 2024},
  url = {https://www.getty.edu/research/tools/vocabularies/aat/about.html},
  urldate = {2025-09-03}
}

@online{gettyresearchinstituteCONAIA2024,
  title = {About {{CONA}} and {{IA}}},
  author = {{Getty Research Institute}},
  year = {révisé le 31 juillet 2024},
  url = {https://www.getty.edu/research/tools/vocabularies/cona/about.html#history_ia},
  urldate = {2025-09-03}
}

@online{gettyresearchinstituteGettyVocabularies,
  title = {Getty {{Vocabularies}}},
  author = {{Getty Research Institute}},
  url = {https://www.getty.edu/research/tools/vocabularies/},
  urldate = {2025-09-03}
}

@online{gianeselliInterrogerBaseLigne2011,
  title = {Interroger la base en ligne les peintures italiennes en France (RETIF) sur Agorha (INHA)},
  author = {Gianeselli, Matteo and Tchernia, Marie and Volle, Nathalie},
  date = {2011-12-08},
  url = {http://blog.apahau.org/interroger-la-base-en-ligne-les-peintures-italiennes-en-france-retif-sur-agorha-inha/},
  urldate = {2025-08-08},
  langid = {french},
  organization = {Blog de l'APAHAU}
}

@online{googleBienvenueDansColab,
  title = {Bienvenue dans Colab},
  author = {{Google}},
  url = {https://colab.research.google.com/?hl=fr},
  urldate = {2025-09-01},
  langid = {french}
}

@online{googlelabsGoogleNotebookLM,
  title = {Google NotebookLM},
  author = {{Google Labs}},
  url = {https://notebooklm.google/},
  urldate = {2025-09-02},
  abstract = {Découvrez NotebookLM, l'outil de recherche et le partenaire de réflexion par IA capable d'analyser vos sources, de clarifier des thèmes complexes et de transformer votre contenu.},
  langid = {french}
}

@online{haffendenUnearthingForgottenImages2023,
  title = {Unearthing forgotten images with the help of {{AI}}},
  author = {Haffenden, Chris and Rekathati, Faton and Rende, Emma},
  date = {2023-10-20},
  url = {https://kb-labb.github.io/posts/2023-10-20-unearthing-forgotten-images-with-the-help-of-ai},
  urldate = {2025-09-04},
  abstract = {How can new AI methods be used to improve the searchability and accessibility of visual heritage collections? We’ve taken advantage of the possibilities opened up by multimodal AI models to produce a demo showcasing these capabilities via the example of postcards. Here we explain something about the project behind this and encourage you to explore our image search demo! https://lab.kb.se/bildsok},
  langid = {english},
  organization = {The KBLab Blog}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  urldate = {2025-08-20},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \&amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{heitzingerImprovingSemanticSegmentation2022,
  title = {Improving semantic segmentation of fine art images using photographs rendered in a style learned from artworks},
  author = {Heitzinger, Thomas and Stork, David G.},
  date = {2022-01-16},
  journaltitle = {IS\&T International Symposium on Electronic Imaging 2022},
  volume = {34},
  number = {13},
  pages = {169-1-169-5},
  issn = {2470-1173},
  doi = {10.2352/EI.2022.34.13.CVAA-169},
  url = {https://library.imaging.org/ei/articles/34/13/CVAA-169},
  urldate = {2025-08-07}
}

@online{HikarIA,
  title = {{{HikarIA}}},
  url = {https://hikaria.org/},
  urldate = {2025-08-18}
}

@online{HistoryIndex,
  title = {History of the {{Index}}},
  url = {https://ima.princeton.edu/history/},
  urldate = {2025-08-27},
  langid = {english},
  organization = {The Index of Medieval Art}
}

@online{huma-numInfrastructureMulticiblesPour2025,
  title = {Infrastructure multi-cibles pour l’usage de l’IA en sciences humaines et sociales},
  author = {{Huma-Num}},
  date = {2025-08-27},
  url = {https://documentation.huma-num.fr/infrastructure-IA/},
  urldate = {2025-09-01},
  abstract = {Documentation de l'infrastructure multi-cibles pour l'usage de l'IA en sciences humaines et sociales (IA de traitement et IA générative)},
  langid = {french},
  organization = {Documentation de l'infrastructure Huma-Num}
}

@misc{hummFascinatingOpenData2020,
  title = {Fascinating with {{Open Data}}: {{openArtBrowser}}},
  author = {Humm, Bernhard G.},
  date = {2020},
  url = {https://ceur-ws.org/Vol-2535/paper_2.pdf},
  urldate = {2025-08-27}
}

@online{hummOpenArtBrowser,
  title = {{{openArtBrowser}}},
  author = {Humm, Bernhard G.},
  url = {https://openartbrowser.org/fr/},
  urldate = {2025-09-03}
}

@online{impettImageGraph,
  title = {{{ImageGraph}}},
  author = {Impett, Leonardo},
  url = {https://leoimpett.github.io/imagegraph-docs/index.html},
  urldate = {2025-09-04}
}

@online{inseeEmissionsGazEffet2025,
  title = {Émissions de gaz à effet de serre par habitant},
  author = {{Insee}},
  date = {2025-01-06},
  url = {https://www.insee.fr/fr/outil-interactif/5367857/details/90_DDE/92_DEV/92G_Figure7},
  urldate = {2025-08-30},
  organization = {Tableau de bord de l'économie française}
}

@online{institutnationaldhistoiredelartAGORHA,
  title = {{{AGORHA}}},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/},
  urldate = {2025-09-03}
}

@online{institutnationaldhistoiredelartEnvoisRomePeinture,
  title = {Les {{Envois}} de {{Rome}} en peinture et sculpture, 1804-1914},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/80},
  urldate = {2025-09-01},
  organization = {AGORHA}
}

@online{institutnationaldhistoiredelartFabriqueLartCouleurs,
  title = {La fabrique de l'art. {{Couleurs}} et matériaux de l'enluminure},
  author = {{Institut national d'histoire de l'art} and {Bibliothèque nationale de France}},
  url = {https://agorha.inha.fr/database/89},
  urldate = {2025-09-02},
  organization = {INHA}
}

@online{institutnationaldhistoiredelartIconographieMusicaleRepertoire,
  title = {Iconographie musicale : répertoire d'œuvres d'art à sujets musicaux publiées par {{Albert Pomme}} de {{Mirimonde}}},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/17},
  urldate = {2025-09-01},
  organization = {AGORHA}
}

@online{institutnationaldhistoiredelartRecensementPeintureProduite,
  title = {Recensement de la peinture produite en {{France}} au {{XVIe}} siècle},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/50},
  urldate = {2025-09-01},
  organization = {AGORHA}
}

@online{institutnationaldhistoiredelartRecensementTableauxIberiques,
  title = {Recensement des tableaux ibériques dans les collections publiques françaises (1300-1870), {{RETIB}}},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/49},
  urldate = {2025-09-02},
  organization = {AGORHA}
}

@online{institutnationaldhistoiredelartRepertoireTableauxFrancais,
  title = {Répertoire des tableaux français en {{Allemagne}} ({{XVIIe}} et {{XVIIIe}} siècles), {{REPFALL}}},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/5},
  urldate = {2025-09-01}
}

@online{institutnationaldhistoiredelartRepertoireTableauxItaliens,
  title = {Répertoire des tableaux italiens dans les collections publiques françaises ({{XIIIe-XIXe}} siècles), {{RETIF}} - {{Agorha}}},
  author = {{Institut national d'histoire de l'art}},
  url = {https://agorha.inha.fr/database/4},
  urldate = {2025-09-01},
  organization = {AGORHA}
}

@online{institutnationaldhistoiredelartThesaurusAGORHAGarnier2021,
  title = {Thésaurus {{AGORHA Garnier}}},
  author = {{Institut national d'histoire de l'art}},
  year = {27 septembre 2021, dernière modificiation le 26 août 2025},
  url = {https://thesaurus.inha.fr/thesaurus/page/ark:/54721/b50e6bb5-96f2-41ea-a42f-1155135b6295},
  urldate = {2025-09-03},
  organization = {Thésaurus et vocabulaires contrôlés de l'Institut national d'histoire de l'art}
}

@online{institutnationaldhistoiredelartThesaurusVocabulairesControles,
  title = {Thésaurus et vocabulaires contrôlés de l'Institut national d'histoire de l'art},
  author = {{Institut national d'histoire de l'art}},
  url = {http://thesaurus.inha.fr/thesaurus/},
  urldate = {2025-08-26},
  langid = {french}
}

@unpublished{kermorvantProjetHikarIAAnalyse2025,
  type = {Les Lundis du Numériques de l'INHA},
  title = {Le projet HikarIA : analyse iconographique automatisée de photographies patrimoniales},
  shorttitle = {Le projet HikarIA},
  author = {Kermorvant, Christopher and family=Saint-Ours, given=Édouard, prefix=de, useprefix=true},
  date = {2025-06-16},
  url = {https://www.inha.fr/agenda/le-projet-hikaria-analyse-iconographique-automatisee-de-photographies-patrimoniales/},
  langid = {french},
  venue = {Institut national d'histoire de l'art}
}

@online{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date = {2023},
  doi = {10.48550/ARXIV.2304.02643},
  url = {https://arxiv.org/abs/2304.02643},
  urldate = {2025-09-01},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@online{labordeBasesDonneesLINHA2021,
  title = {Les bases de données de l’INHA, une longue histoire},
  author = {Laborde, Pierre-Yves},
  date = {2021-03-18},
  url = {https://numrha.hypotheses.org/2164},
  urldate = {2025-07-27},
  langid = {french},
  organization = {Numrha}
}

@online{liBLIPBootstrappingLanguageImage2022,
  title = {{{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}},
  shorttitle = {{{BLIP}}},
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  date = {2022},
  doi = {10.48550/ARXIV.2201.12086},
  url = {https://arxiv.org/abs/2201.12086},
  urldate = {2025-09-02},
  abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{libraryofcongressPlanificationProjetIA2025,
  title = {Planification de projet {{IA}}, traduction de l'{{Artificial Intelligence Planning Framework}} de la {{Library}} of {{Congress}}},
  translator = {{Library of Congress} and Moreux, Jean-Philippe},
  date = {2025-05-23T13:32:07Z},
  origdate = {2024-05-25T09:42:47Z},
  url = {https://github.com/altomator/Planification-de-projets-IA},
  urldate = {2025-09-01},
  abstract = {Traduction du "LC Labs Artificial Intelligence Planning Framework"},
  organization = {Github}
}

@online{libraryofcongressPlanningFrameworkUsed2025,
  title = {Planning {{Framework}} used by {{LC Labs}} for planning {{AI}} experiments towards responsible implementation},
  author = {{Library of Congress}},
  date = {2025-07-25T11:49:43Z},
  origdate = {2023-11-03T18:23:35Z},
  url = {https://github.com/LibraryOfCongress/labs-ai-framework},
  urldate = {2025-09-01},
  abstract = {Planning Framework used by LC Labs for planning AI experiments towards responsible implementation},
  organization = {Github}
}

@online{libraryofcongressThesaurusGraphicMaterials2024,
  title = {Thesaurus for Graphic Materials I (TGM): An Introduction},
  author = {{Library of Congress}},
  date = {2024},
  url = {https://guides.loc.gov/tgm-i},
  urldate = {2025-08-27},
  langid = {french}
}

@online{LIMCFrance2003,
  title = {{{LIMC-France}}},
  date = {2003},
  url = {https://heurist.huma-num.fr/LIMC_FRANCE/web/293436},
  urldate = {2025-08-26}
}

@online{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  date = {2023},
  doi = {10.48550/ARXIV.2304.08485},
  url = {https://arxiv.org/abs/2304.08485},
  urldate = {2025-09-02},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@online{martinGoogleNotebookLM2023,
  title = {Google {{NotebookLM}}},
  author = {Martin, Raiza and Johnson, Steven},
  date = {2023-07-12},
  url = {https://blog.google/technology/ai/notebooklm-google-ai/},
  urldate = {2025-09-02},
  abstract = {Découvrez NotebookLM, l'outil de recherche et le partenaire de réflexion par IA capable d'analyser vos sources, de clarifier des thèmes complexes et de transformer votre contenu.},
  organization = {Google}
}

@book{metropolitanmuseumofartComputersTheirPotential1968,
  title = {Computers and their potential applications in museums. {{Conférences}} organisées les 15, 16 et 17 avril 1968 avec le soutien d'{{IBM}}.},
  author = {{Metropolitan Museum of Art}},
  date = {1968},
  publisher = {Arno Press},
  location = {New York}
}

@online{milesSKOSSimpleKnowledge2009,
  title = {{{SKOS}} ({{Simple Knowledge Organization System Reference}})},
  author = {Miles, Alistair and Bechhofer, Sean},
  date = {2009-08-18},
  url = {https://www.w3.org/TR/skos-reference/},
  urldate = {2025-09-03}
}

@online{ministeredelacultureComparIAComparateur2024,
  title = {compar:IA, Le comparateur d'IA conversationnelles},
  author = {{Ministère de la Culture}},
  date = {2024-10},
  url = {https://www.comparia.beta.gouv.fr/},
  urldate = {2025-08-31},
  abstract = {compar:IA est un outil permettant de comparer à l’aveugle différents modèles d'IA conversationnelle pour sensibiliser aux enjeux de l'IA générative (biais, impact environmental) et constituer des jeux de données de préférence en français.},
  langid = {french}
}

@online{ministeredelaculturePOPPlateformeOuverte,
  title = {« POP », la plate-forme ouverte du patrimoine},
  author = {{Ministère de la Culture}},
  url = {https://www.culture.gouv.fr/fr/espace-documentation/bases-de-donnees/Fiches-bases-de-donnees/pop-la-plate-forme-ouverte-du-patrimoine},
  urldate = {2025-08-26},
  langid = {french},
  organization = {Ministère de la Culture}
}

@online{ministeredelacultureThesaurusIconographiqueGarnier,
  title = {Thésaurus iconographique {{Garnier}}},
  author = {{Ministère de la Culture}},
  url = {https://opentheso.huma-num.fr/?idt=th285},
  urldate = {2025-08-20},
  organization = {Opentheso}
}

@online{ministeredelacultureThesaurusIconographiqueSysteme2020,
  title = {Thésaurus iconographique, système descriptif des représentations de François Garnier},
  author = {{Ministère de la Culture}},
  date = {2020-06-04},
  url = {https://www.culture.gouv.fr/thematiques/musees/pour-les-professionnels/conserver-et-gerer-les-collections/informatiser-les-collections-d-un-musee-de-france/vocabulaires-scientifiques-du-service-des-musees-de-france/thesaurus-iconographique-systeme-descriptif-des-representations-de-francois-garnier},
  urldate = {2025-08-26},
  langid = {french},
  organization = {Musées}
}

@online{mistralaiMediumNewLarge,
  title = {Medium is the new large},
  author = {{Mistral AI}},
  url = {https://mistral.ai/fr/news/mistral-medium-3},
  urldate = {2025-09-02},
  abstract = {Mistral Medium 3 delivers state-of-the-art performance at 10X lower cost with radically simplified enterprise deployments.},
  langid = {french}
}

@online{mistralaiMistral7B2023,
  title = {Mistral 7B},
  author = {{Mistral AI}},
  date = {2023-09-27},
  url = {https://mistral.ai/fr/news/announcing-mistral-7b},
  urldate = {2025-09-02},
  abstract = {The best 7B model to date, Apache 2.0},
  langid = {french},
  organization = {Mistral.ai}
}

@online{mistralaiOurContributionGlobal2025,
  title = {Our contribution to a global environmental standard for {{AI}}},
  author = {{Mistral AI}},
  date = {2025-07-22},
  url = {https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai},
  urldate = {2025-08-31}
}

@online{museedereimsOeuvresLigne,
  title = {Les oeuvres en ligne},
  author = {{Musée de Reims}},
  url = {https://musees-reims.fr/fr/musee-numerique/oeuvres-en-ligne},
  urldate = {2025-09-03},
  langid = {french},
  organization = {Portail officiel des Musées de Reims}
}

@online{museedorsayCollections,
  title = {Les collections},
  author = {{Musée d'Orsay}},
  url = {https://www.musee-orsay.fr/fr/collections},
  urldate = {2025-09-03}
}

@online{NotesVersionHikaria,
  title = {Notes de version - {{Hikaria}}},
  url = {https://hikaria.org/about/release-notes},
  urldate = {2025-09-01},
  organization = {HikarIA}
}

@book{okayamaRipaIndexPersonifications1992,
  title = {The {{Ripa}} index: personifications and their attributes in five editions of the {{Iconologia}}},
  author = {Okayama, Yassu},
  date = {1992},
  publisher = {Davaco Publishers},
  location = {Doornspijk},
  pagetotal = {628}
}

@online{openaiGPT4,
  title = {GPT-4},
  author = {{OpenAI}},
  url = {https://openai.com/fr-FR/index/gpt-4/},
  urldate = {2025-09-02},
  abstract = {Ce système peut générer du contenu, le modifier et collaborer avec les utilisateurs sur des tâches de rédaction technique et créative, comme l'écriture de paroles d'une chanson ou de scénarios, ou même assimiler le style d'écriture d'un utilisateur.},
  langid = {french},
  organization = {OpenAI}
}

@online{oquabDINOv2LearningRobust2023,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  date = {2023},
  doi = {10.48550/ARXIV.2304.07193},
  url = {https://arxiv.org/abs/2304.07193},
  urldate = {2025-09-01},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{P180DepeintDepicts,
  title = {P180 (dépeint / depicts)},
  url = {https://www.wikidata.org/wiki/Property:P180},
  urldate = {2025-09-03},
  abstract = {entité visuellement dépeinte dans une image, décrite littéralement décrite dans une œuvre, ou incorporée dans un médium audio-visuel ou autre; voir aussi P921 "sujet ou thème principal"},
  langid = {french},
  organization = {Wikidata}
}

@book{panofskyEssaisDiconologieThemes2021,
  title = {Essais d'iconologie: thèmes humanistes dans l'art de la Renaissance},
  shorttitle = {Essais d'iconologie},
  author = {Panofsky, Erwin},
  translator = {Herbette, Claude and Teyssèdre, Bernard},
  date = {2021},
  series = {Tel},
  edition = {Nouvelle éd.},
  number = {436},
  publisher = {Gallimard},
  location = {Paris},
  abstract = {Quand les Essais d'iconologie sont réunis pour la première fois, en 1939, l'histoire de l'art est encore dominée par des explications psychologiques et esthétiques, voire réduite à une simple description iconographique. C'est en approfondissant les démarches de ses maîtres Ernst Cassirer et Aby Warburg qu'Erwin Panofsky parvient à lui offrir son statut contemporain de science d'interprétation. À partir d'un corpus d'œuvres picturales traversant plusieurs siècles du Moyen Âge à la Renaissance, l'historien retrace les métamorphoses des traditions antiques : le temps, l'amour, la mort ou la genèse du monde ; il met au jour de petites odyssées étranges et savantes, dont les épisodes consistent en fusions et confusions de concepts et d'images, en malentendus, oublis, et renaissances de toute sorte. En décodant ces petites « résurrections » humanistes, Panofsky donne accès à une compréhension de la chimie artistique où la pensée imageante prend le pas sur la pensée discursive},
  isbn = {978-2-07-295767-3},
  langid = {fre}
}

@article{panofskyProblemBeschreibungUnd1932,
  title = {Zum {{Problem}} der {{Beschreibung}} und {{Inhaltsdeutung}} von {{Werken}} der bildenden {{Kunst}}},
  author = {Panofsky, Erwin},
  date = {1932},
  journaltitle = {Logos},
  number = {21},
  pages = {103--119}
}

@online{perezWikipediasNextBig2012,
  title = {Wikipedia's {{Next Big Thing}}: {{Wikidata}}, {{A Machine-Readable}}, {{User-Editable Database Funded By Google}}, {{Paul Allen And Others}}},
  shorttitle = {Wikipedia's {{Next Big Thing}}},
  author = {Perez, Sarah},
  date = {2012-03-30T10:00:21+00:00},
  url = {https://techcrunch.com/2012/03/30/wikipedias-next-big-thing-wikidata-a-machine-readable-user-editable-database-funded-by-google-paul-allen-and-others/},
  urldate = {2025-09-03},
  abstract = {Wikidata, the first new project to emerge from the Wikimedia Foundation since 2006, is now beginning development. The organization, known best for its user-edited encyclopedia of knowledge~Wikipedia, recently announced the new project at February's~Semantic Tech \& Business Conference~in Berlin, describing Wikidata as new effort to provide a database of knowledge that can be read and edited by humans and machines alike.  There have been other attempts at creating a semantic database built from Wikipedia's data before - for example, DBpedia, a community effort to extract structured content from Wikipedia and make it available online. The difference is that, with Wikidata, the data won't just be made available, it will also be made editable by anyone.},
  langid = {american},
  organization = {TechCrunch}
}

@book{petersenArtArchitectureThesaurus1990,
  title = {Art \& {{Architecture Thesaurus}}},
  editor = {Petersen, Toni},
  editortype = {redactor},
  year = {1990, 2e édition 1994},
  publisher = {Oxford University Press},
  location = {New York}
}

@software{posthumusArkyves1999,
  title = {Arkyves},
  author = {Posthumus, Etienne and Brandhorst, Hans},
  date = {1999},
  url = {https://brill.com/display/db/arko},
  urldate = {2025-09-03},
  abstract = {"Arkyves" published on  by Brill.}
}

@online{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021},
  doi = {10.48550/ARXIV.2103.00020},
  url = {https://arxiv.org/abs/2103.00020},
  urldate = {2025-08-30},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@online{ramosNoAnnotationsObject2024,
  title = {No {{Annotations}} for {{Object Detection}} in {{Art}} through {{Stable Diffusion}}},
  author = {Ramos, Patrick and Gonthier, Nicolas and Khan, Selina and Nakashima, Yuta and Garcia, Noa},
  date = {2024},
  doi = {10.48550/ARXIV.2412.06286},
  url = {https://arxiv.org/abs/2412.06286},
  urldate = {2025-08-20},
  abstract = {Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@online{rdfworkinggroupResourceDescriptionFramework2014,
  title = {Resource {{Description Framework}} ({{RDF}})},
  author = {{RDF Working Group}},
  date = {2014-02-25},
  url = {https://www.w3.org/RDF/},
  urldate = {2025-09-03}
}

@online{resigSite2012,
  title = {About {{The Site}}},
  author = {Resig, John},
  date = {2012},
  url = {https://ukiyo-e.org/about},
  urldate = {2025-09-04},
  organization = {Ukiyo-e.org}
}

@online{resigUkiyoeorgJapanesePrint2012,
  title = {Ukiyo-e.org : {{Japanese Print Search}} and {{Database}}},
  author = {Resig, John},
  date = {2012},
  url = {https://ukiyo-e.org/},
  urldate = {2025-09-04}
}

@inproceedings{richardAnalyseIconographiqueFonds1989,
  title = {Analyse iconographique des fonds d’images abondants et redondants},
  booktitle = {A l'écoute de l'œil, {{Les}} collections iconographiques et les bibliothèques ({{Genève}}, {{Section}} des {{Bibliothèques}} d'{{Art}} de l'{{IFLA}}, 13-15 mars 1985)},
  author = {Richard, Philippe and Lozza, Brigitte},
  date = {1989},
  pages = {156--165},
  publisher = {K.G. Saur},
  location = {Munich, New York, Londres, Paris}
}

@book{ripaIconologia1593,
  title = {Iconologia},
  author = {Ripa, Cesare},
  year = {1593, 1603},
  location = {Rome}
}

@online{RoboflowComputerVision,
  title = {Roboflow: {{Computer}} vision tools for developers and enterprises},
  url = {https://roboflow.com/},
  urldate = {2025-09-04}
}

@book{rouitLecouteLoeilCollections1989,
  title = {A l’écoute de l’oeil: les collections iconographiques et les bibliothèques. Actes du colloque organisé par la Section des Bibliothèques d’Art de l’IFLA, Genève, 13-15 mars 1985},
  shorttitle = {À l’écoute de l’oeil},
  author = {Rouit, Huguette and Dubouloz, Jean-Pierre and {al.}},
  date = {1989},
  series = {IFLA Publications},
  edition = {K.G. Saur},
  number = {47},
  publisher = {K.G. Saur},
  location = {Munich},
  langid = {french}
}

@online{ryknerBasesDonneesLINHA2011,
  title = {Les bases de données de l’{{INHA}} regroupées pour un résultat mitigé},
  author = {Rykner, Didier},
  date = {2011-04-29},
  url = {https://www.latribunedelart.com/les-bases-de-donnees-de-l-inha-regroupees-pour-un-resultat-mitige},
  urldate = {2025-08-27},
  organization = {La Tribune de l'Art}
}

@online{santiniMultimodalSearchIconclass2023,
  title = {Multimodal {{Search}} on {{Iconclass}} using {{Vision-Language Pre-Trained Models}}},
  author = {Santini, Cristian and Posthumus, Etienne and Tan, Mary Ann and Bruns, Oleksandra and Tietz, Tabea and Sack, Harald},
  date = {2023},
  doi = {10.48550/ARXIV.2306.16529},
  url = {https://arxiv.org/abs/2306.16529},
  urldate = {2025-09-04},
  abstract = {Terminology sources, such as controlled vocabularies, thesauri and classification systems, play a key role in digitizing cultural heritage. However, Information Retrieval (IR) systems that allow to query and explore these lexical resources often lack an adequate representation of the semantics behind the user's search, which can be conveyed through multiple expression modalities (e.g., images, keywords or textual descriptions). This paper presents the implementation of a new search engine for one of the most widely used iconography classification system, Iconclass. The novelty of this system is the use of a pre-trained vision-language model, namely CLIP, to retrieve and explore Iconclass concepts using visual or textual queries.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Digital Libraries (cs.DL),FOS: Computer and information sciences,Information Retrieval (cs.IR)}
}

@incollection{seguinVisualLinkRetrieval2016,
  title = {Visual {{Link Retrieval}} in a {{Database}} of {{Paintings}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016 {{Workshops}}},
  author = {Seguin, Benoit and Striolo, Carlotta and family=Lenardo, given=Isabella, prefix=di, useprefix=true and Kaplan, Frederic},
  date = {2016},
  volume = {9913},
  pages = {753--767},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-46604-0_52},
  url = {http://link.springer.com/10.1007/978-3-319-46604-0_52},
  urldate = {2025-08-20},
  isbn = {978-3-319-46603-3 978-3-319-46604-0},
  langid = {english}
}

@article{shatfordAnalyzingSubjectPicture1986,
  title = {Analyzing the {{Subject}} of a {{Picture}}: {{A Theoretical Approach}}},
  author = {Shatford, Sara},
  date = {1986},
  journaltitle = {Cataloging \& Classification Quarterly},
  volume = {6},
  number = {3},
  pages = {39--62}
}

@article{shatfordDescribingPictureThousand1984,
  title = {Describing a {{Picture}}: {{A Thousand Words}} are {{Seldom Cost Effective}}},
  author = {Shatford, Sara},
  date = {1984},
  journaltitle = {Cataloging \& Classification Quarterly},
  volume = {4},
  number = {4},
  pages = {13--30}
}

@online{shenDiscoveringVisualPatterns2019,
  title = {Discovering {{Visual Patterns}} in {{Art Collections}} with {{Spatially-consistent Feature Learning}}},
  author = {Shen, Xi and Efros, Alexei A. and Aubry, Mathieu},
  date = {2019},
  doi = {10.48550/ARXIV.1903.02678},
  url = {https://arxiv.org/abs/1903.02678},
  urldate = {2025-09-04},
  abstract = {Our goal in this paper is to discover near duplicate patterns in large collections of artworks. This is harder than standard instance mining due to differences in the artistic media (oil, pastel, drawing, etc), and imperfections inherent in the copying process. The key technical insight is to adapt a standard deep feature to this task by fine-tuning it on the specific art collection using self-supervised learning. More specifically, spatial consistency between neighbouring feature matches is used as supervisory fine-tuning signal. The adapted feature leads to more accurate style-invariant matching, and can be used with a standard discovery approach, based on geometric verification, to identify duplicate patterns in the dataset. The approach is evaluated on several different datasets and shows surprisingly good qualitative discovery results. For quantitative evaluation of the method, we annotated 273 near duplicate details in a dataset of 1587 artworks attributed to Jan Brueghel and his workshop. Beyond artwork, we also demonstrate improvement on localization on the Oxford5K photo dataset as well as on historical photograph localization on the Large Time Lags Location (LTLL) dataset.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{shenSpatiallyConsistentFeatureMatching2022,
  title = {Spatially-{{Consistent Feature Matching}} and {{Learning}} for {{Heritage Image Analysis}}},
  author = {Shen, Xi and Champenois, Robin and Ginosar, Shiry and Pastrolin, Ilaria and Rousselot, Morgane and Bounou, Oumayma and Monnier, Tom and Gidaris, Spyros and Bougard, François and Raverdy, Pierre-Guillaume and Limon, Marie-Françoise and Bénévent, Christine and Smith, Marc and Poncet, Olivier and Bender, K. and Joyeux-Prunel, Béatrice and Honig, Elizabeth and Efros, Alexei A. and Aubry, Mathieu},
  date = {2022-05},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {130},
  number = {5},
  pages = {1325--1339},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-022-01576-x},
  url = {https://link.springer.com/10.1007/s11263-022-01576-x},
  urldate = {2025-09-04},
  langid = {english}
}

@online{singhFLAVAFoundationalLanguage2021,
  title = {{{FLAVA}}: {{A Foundational Language And Vision Alignment Model}}},
  shorttitle = {{{FLAVA}}},
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  date = {2021},
  doi = {10.48550/ARXIV.2112.04482},
  url = {https://arxiv.org/abs/2112.04482},
  urldate = {2025-08-20},
  abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{smitsMultimodalTurnDigital2023,
  title = {A multimodal turn in {{Digital Humanities}}. {{Using}} contrastive machine learning models to explore, enrich, and analyze digital visual historical collections},
  author = {Smits, Thomas and Wevers, Melvin},
  date = {2023-08-31},
  journaltitle = {Digital Scholarship in the Humanities},
  volume = {38},
  number = {3},
  pages = {1267--1280},
  issn = {2055-7671, 2055-768X},
  doi = {10.1093/llc/fqad008},
  url = {https://academic.oup.com/dsh/article/38/3/1267/7078540},
  urldate = {2025-08-07},
  abstract = {Abstract             Until recently, most research in the Digital Humanities (DH) was monomodal, meaning that the object of analysis was either textual or visual. Seeking to integrate multimodality theory into the DH, this article demonstrates that recently developed multimodal deep learning models, such as Contrastive Language Image Pre-training (CLIP), offer new possibilities to explore and analyze image–text combinations at scale. These models, which are trained on image and text pairs, can be applied to a wide range of text-to-image, image-to-image, and image-to-text prediction tasks. Moreover, multimodal models show high accuracy in zero-shot classification, i.e. predicting unseen categories across heterogeneous datasets. Based on three exploratory case studies, we argue that this zero-shot capability opens up the way for a multimodal turn in DH research. Moreover, multimodal models allow scholars to move past the artificial separation of text and images that was dominant in the field and analyze multimodal meaning at scale. However, we also need to be aware of the specific (historical) bias of multimodal deep learning that stems from biases in the training data used to train these models.},
  langid = {english}
}

@online{sologubSketchyCollectionsExploring2024,
  title = {Sketchy {{Collections}}: {{Exploring Digital Museum Collections}} by {{Drawing}} via {{CLIP}}},
  author = {Sologub, Polo and Fiebrink, Rebecca},
  date = {2024},
  url = {https://ualresearchonline.arts.ac.uk/id/eprint/23541/1/NeurIPS_2024_Sketchy_Collections_camera-ready.pdf},
  urldate = {2025-09-04},
  abstract = {In recent years, digital museum collections have made it possible for everyone to discover cultural heritage (CH) online. However, that does not mean that they are engaging or fun for casual users to explore. In this paper, we develop a web interface that lets users search and compare three museum collections by drawing images. We describe our approach of using CLIP as a feature extraction model for a Sketch-Based Image Retrieval (SBIR) model based on museum tags. Through qualitative experiments and a user study, we demonstrate that the model performs well in a CH context with interesting results and that the interface enables playful search and serendipitous discoveries.},
  langid = {english},
  pubstate = {prepublished}
}

@article{storkComputerVisionML2024,
  title = {Computer {{Vision}}, {{ML}}, and {{AI}} in the {{Study}} of {{Fine Art}}},
  author = {Stork, David G.},
  date = {2024-05},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {67},
  number = {5},
  pages = {68--75},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3633454},
  url = {https://dl.acm.org/doi/10.1145/3633454},
  urldate = {2025-08-07},
  abstract = {Immense challenges and opportunities remain for the application of AI in the study of fine-art paintings and drawings.},
  langid = {english}
}

@book{storkPixelsPaintingsFoundations2024,
  title = {Pixels \& paintings: foundations of computer-assisted connoisseurship},
  shorttitle = {Pixels \& paintings},
  author = {Stork, David G.},
  date = {2024},
  publisher = {Wiley},
  location = {Hoboken, New Jersey},
  isbn = {978-0-470-22944-6 978-1-394-18664-8},
  langid = {english},
  pagetotal = {711}
}

@online{streamlitDocumentation,
  title = {Documentation},
  author = {{Streamlit}},
  url = {https://docs.streamlit.io/},
  urldate = {2025-09-04},
  organization = {Streamlit}
}

@online{tekliaArkindexDocumentProcessing,
  title = {Arkindex, the document processing platform},
  author = {{TEKLIA}},
  url = {https://teklia.com/our-solutions/arkindex/},
  urldate = {2025-09-02},
  organization = {TEKLIA}
}

@book{thewarburginstituteSummaryGuidePhotographic1988,
  title = {Summary guide to the photographic collection of the {{Warburg Institute}}},
  author = {{The Warburg Institute}},
  date = {1988},
  publisher = {University of London},
  location = {Londres}
}

@inproceedings{thuillierImageInformatiqueLutilisation1989,
  title = {Image et informatique : de l'utilisation au système},
  booktitle = {A l’écoute de l’oeil: les collections iconographiques et les bibliothèques. {{Actes}} du colloque organisé par la {{Section}} des {{Bibliothèques}} d’{{Art}} de l’{{IFLA}}, {{Genève}}, 13-15 mars 1985},
  author = {Thuillier, Jacques},
  date = {1989},
  pages = {255--261},
  publisher = {K.G. Saur},
  location = {Munich}
}

@article{thuillierLinformatiqueHistoireLart1992,
  title = {L'informatique en histoire de l'art : où en sommes-nous ?},
  shorttitle = {L'informatique en histoire de l'art},
  author = {Thuillier, Jacques},
  date = {1992},
  journaltitle = {Revue de l'Art},
  shortjournal = {rvart},
  volume = {97},
  number = {3},
  pages = {5--10},
  issn = {0035-1326},
  doi = {10.3406/rvart.1992.347996},
  url = {https://www.persee.fr/doc/rvart_0035-1326_1992_num_97_1_347996},
  urldate = {2025-08-18},
  langid = {french}
}

@article{thuillierLinformatiqueLhistoireLart1997,
  title = {L'informatique et l'histoire de l'art},
  author = {Thuillier, Jacques},
  date = {1997},
  journaltitle = {Revue de l'Art},
  shortjournal = {rvart},
  volume = {117},
  number = {3},
  pages = {5--8},
  issn = {0035-1326},
  doi = {10.3406/rvart.1997.348337},
  url = {https://www.persee.fr/doc/rvart_0035-1326_1997_num_117_1_348337},
  urldate = {2025-08-18},
  langid = {french}
}

@software{ultralyticsYOLOv8YOLOv112025,
  title = {{{YOLOv8}} / {{YOLOv11}}},
  author = {{Ultralytics}},
  date = {2025},
  origdate = {2022-09-11T16:39:45Z},
  url = {https://github.com/ultralytics/ultralytics},
  urldate = {2025-08-31},
  abstract = {Ultralytics YOLO 🚀}
}

@online{universitepariscitePlateformeCalculHautePerformance,
  title = {Plateforme de Calcul Haute-Performance NOVA},
  author = {{Université Paris Cité}},
  url = {https://plateformes.u-paris.fr/plateforme-de-calcul-haute-performance-nova/},
  urldate = {2025-09-01},
  langid = {french},
  organization = {Plateformes \& plateaux - Université Paris Cité}
}

@book{vandewaalDecimalIndexArt1968,
  title = {Decimal {{Index}} of the {{Art}} of the {{Low Countries D}}.{{I}}.{{A}}.{{L}}., {{Abridged}} edition of the {{Iconclass System}}},
  author = {family=Waal, given=Henri, prefix=van de, useprefix=true},
  date = {1968},
  location = {La Haye}
}

@mvbook{vandewaalIconclassIconographicClassification1974,
  title = {Iconclass an iconographic classification system},
  author = {family=Waal, given=Henri, prefix=van de, useprefix=true and Couprie, Leendert D.},
  date = {1974/1985},
  edition = {completed and edited by L.D. Couprie with R.H. Fuchs, E. Tholen \& G. Vellekoop},
  location = {North-Holland, Amsterdam},
  volumes = {17}
}

@inproceedings{vandewaalSystemeClassificationIconographique1969,
  title = {Système de classification iconographique “{{ICONCLASS}}” et la collection de reproductions {{D}}.{{I}}.{{A}}.{{L}}. ({{Decimal Index}} of the {{Art}} of the {{Low Countries}})},
  booktitle = {Colloques {{Internationaux}} du {{Centre National}} de la {{Recherche Scientifique}}},
  author = {family=Waal, given=Henri, prefix=van de, useprefix=true},
  year = {24, 25, 26 mars 1969},
  series = {Bibliographie d'{{Histoire}} de l'{{Art}}},
  pages = {187--190},
  publisher = {C.N.R.S.},
  location = {Paris}
}

@book{vanstratenIconographyIndexingIconclass1994,
  title = {Iconography indexing iconclass: a handbook},
  shorttitle = {Iconography indexing iconclass},
  author = {family=Straten, given=Roelof, prefix=van, useprefix=true},
  date = {1994},
  publisher = {Foleor},
  location = {Leyde},
  isbn = {978-90-75035-03-2},
  langid = {english}
}

@book{vanstratenIntroductionIconography1994,
  title = {An {{Introduction}} to {{Iconography}}},
  author = {family=Straten, given=Roelof, prefix=van, useprefix=true},
  translator = {family=Man, given=Patricia, prefix=de, useprefix=true},
  date = {1994},
  publisher = {{Gordon and Breach}},
  location = {Yverdon (Suisse)}
}

@article{vanstratenPanofskyICONCLASS1986,
  title = {Panofsky and {{ICONCLASS}}},
  author = {family=Straten, given=Roelof, prefix=van, useprefix=true},
  date = {1986},
  journaltitle = {Artibus et Historiae},
  shortjournal = {Artibus et Historiae},
  volume = {7},
  number = {13},
  pages = {165--181},
  issn = {03919064},
  doi = {10.2307/1483254}
}

@online{volleMichelLaclotteRepertoire2024,
  title = {Michel Laclotte et le Répertoire des Tableaux Italiens en France, XIIIe-XIXe siècles (RETIF)},
  author = {Volle, Nathalie},
  date = {2024-02-05},
  url = {https://agorha.inha.fr/detail/940},
  urldate = {2025-08-08},
  abstract = {Lors de l’hommage rendu à Michel Laclotte (1929-2021) à l’Auditorium du musée du Louvre le~22 avril 2022, il a été rappelé l’importance du programme de recherche sur les tableaux italiens~des collections publiques françaises (RETIF) mis en œuvre par ce dernier pendant près de vingt~ans à l’Institut National d’Histoire de l’Art. Cette base est considérée aujourd’hui comme « l’épine dorsale »~des ressources numériques de l’Institut national d’histoire de l’art (INHA). Elle comporte en effet près de~14 000 œuvres recensées dans la base AGORHA.},
  langid = {french},
  organization = {AGORHA}
}

@online{Wikidata,
  title = {Wikidata},
  url = {https://www.wikidata.org/},
  urldate = {2025-09-03}
}
