En suivant les recommandations du cadre de planification des projets en intelligence artificielle de la Bibliothèque du Congrès, la deuxième étape correspond à l’expérimentation, qui succède à la phase de compréhension des données du projet\footcite{libraryofcongressPlanificationProjetIA2025}. L’approche automatisée de l’indexation iconographique du RETIF n’a pas dépassé ce stade ni atteint la phase de planification, mais les tests effectués permettent néanmoins de dégager des pistes de pratiques à privilégier.

\section{S’appuyer sur d’autres expériences}

Afin de mieux comprendre quelles pratiques pourraient être pertinentes pour le corpus étudié, la première étape consiste à s’intéresser aux choix mis en œuvre dans d’autres projets similaires, en particulier les projets TORNE-H et Royère au Musée des Arts décoratifs, le projet Notre-Dame et le projet HikarIA. 
Parmi ces pistes figurait l’expérimentation du modèle YOLO (You Only Look Once), développé par Ultralytics, performant pour la détection et segmentation d’objets récurrents dans les images\footcite{ultralyticsYOLOv8YOLOv112025}. Ce modèle est fréquemment mobilisé dans des projets de vision par ordinateur, comme le projet Torne-H, qui l’utilise pour analyser les œuvres graphiques du fonds Royère au Musée des Arts Décoratifs\footcite{bermesRepenserCollectionsPatrimoniales2025}. Dans notre cas, YOLO a été testé pour la reconnaissance d’objets dans les natures mortes. Plutôt que de mobiliser directement le corpus du RETIF, nous avons opté pour un corpus plus généraliste, constitué d’images issues des collections du Musée du Louvre. Au total, 193 peintures ont été annotées à l’aide de 45 classes d’objets et d’animaux caractéristiques des natures mortes (par exemple : glass, fruits, musical instrument, dog, cat, knife, etc.). Le modèle retenu pour l’expérimentation était une version YOLOv11-small, entraînée sur 50 époques. Les résultats se sont toutefois révélés décevants : testé sur de nouvelles images, le modèle n’a pas été capable de reconnaître de manière fiable des objets similaires à ceux présents dans le jeu d’entraînement (Métriques : \ref{fig:YOLO1} et \ref{fig:YOLO2}). Plusieurs limites ont été identifiées : d’une part, le nombre de classes trop élevé compliquait significativement l’apprentissage et d’autre part, le volume d’images annotées était insuffisant pour permettre une répartition adéquate des objets et leur reconnaissance. Certaines classes, comme les instruments musicaux, présentaient une diversité de représentations trop importante. En définitive, l’approche d’utiliser YOLO s’est révélée inadaptée à notre problématique spécifique et a donc été abandonnée.
Une autre approche pertinente est celle développée dans le cadre du projet Notre-Dame pour l’indexation de vues 2D et 3D. Cette méthodologie mobilise simultanément trois modèles d’intelligence artificielle, dont la manipulation demeure complexe et requiert à la fois des compétences d’ingénierie spécialisées et une infrastructure technique adaptée. Le processus s’appuie sur l’utilisation successive des modèles DINO, SAM et CLIP\footcite{delucaIntelligenceHumaineCollective2024}. Le modèle DINO (Self-Distillation with No Labels), introduit en 2021 par Facebook AI Research\footcite{caronEmergingPropertiesSelfSupervised2021}, constitue une approche d’apprentissage auto-supervisé appliquée aux images. Contrairement à des modèles supervisés tels que YOLO, qui nécessitent un jeu de données annoté par des experts humains, DINO apprend à identifier et à extraire des représentations visuelles sans recours à l’annotation manuelle. Il repose sur l’architecture des Vision Transformers (ViT), qui consistent à transformer les images en leur appliquant diverses modifications (recadrage, variations de couleurs, etc.), afin d’entraîner le modèle à reconnaître les parties significatives malgré leurs variations visuelles. Cette méthode permet à DINO d’isoler automatiquement des objets et des éléments au sein d’une image, sans entraînement spécifique à la segmentation. Une seconde version, publiée en 2023, a enrichi le modèle grâce à un apprentissage sur un corpus massif de 142 millions d’images et une amélioration de la technologie d'entraînement par distillation interne (\textit{self-distillation})\footcite{oquabDINOv2LearningRobust2023}.
Le modèle SAM (Segment Anything Model), développé par Meta en 2023, est conçu pour la segmentation d’images par génération de masques en réponse à des instructions sous forme de prompts, par exemple un clic, une boîte englobante ou une courte description textuelle\footcite{kirillovSegmentAnything2023}. Dans le cadre du projet Notre-Dame, DINO est utilisé pour vectoriser chaque élément d’une image de la cathédrale, par exemple une statue, un vitrail ou un détail architectural, afin d’identifier des similitudes visuelles, comme la représentation d’une même statue sous différents angles. SAM intervient alors en exploitant les régions d’attention détectées par DINO : celles-ci servent de prompt permettant à SAM de générer un masque précis. Ainsi, dans le cas d’une photographie de la Galerie des Rois, DINO identifie les silhouettes saillantes des statues par rapport à la niche dans laquelle elle se trouve, et SAM affine cette première analyse en produisant le contour exact de chaque figure. Enfin, une troisième étape mobilise le modèle CLIP, entraîné à associer représentations visuelles et langage naturel\footcite{radfordLearningTransferableVisual2021}. Cet outil multimodal permet notamment de relier les objets segmentés aux entrées d’un thésaurus, facilitant ainsi leur indexation dans une perspective documentaire. Dans d’une indexation automatisée du RETIF, l’association des modèles DINO, SAM et éventuellement CLIP pourrait constituer une solution prometteuse pour la détection automatique de motifs, de structures ou d’éléments visuels récurrents dans les peintures, et leur mise en correspondance avec un vocabulaire contrôlé restreint. Toutefois, cette approche n’a pas pu être expérimentée, en raison de la complexité de sa mise en œuvre, qui exige à la fois une expertise technique avancée et une capacité de calcul importante pour l’utilisation et l’articulation de ces modèles.	
Une troisième méthode, moins complexe à mettre en œuvre, consiste à exploiter les capacités des grands modèles de langage (LLM) pour générer des descriptions d’images. Cette approche a notamment été expérimentée dans le cadre du projet HikarIA. Pour décrire les photographies japonaises, trois modèles avaient alors été évalués : CLIP, Qwen2 et GPT-4, ce dernier s’étant distingué par la qualité de ses résultats. Grâce à l’application open source Arkindex, développée par la société TEKLIA, il a été possible d’accéder de manière simple à l’API de GPT-4 afin de traiter les images par l’intermédiaire du modèle de langage\footcite{tekliaArkindexDocumentProcessing}. Près de deux cents images ont ainsi été décrites automatiquement, et les descripteurs générés ont été intégrés au site, avec une icône signalant qu’ils avaient été générés par un outil d’intelligence artificielle. Pour l’indexation iconographique du RETIF, le recours à des LLM avait d’ailleurs été envisagé dès les premières phases du projet. Toutefois, cette option suppose la capacité d’intégrer et de respecter un vocabulaire contrôlé.

\section[Le vocabulaire contrôlé]{Intégrer le vocabulaire contrôlé au processus}

L’intégration d’un vocabulaire contrôlé dans la chaîne de traitement constitue un défi majeur pour l’automatisation de l’indexation iconographique et conditionne le choix de la démarche adoptée. Les systèmes d’intelligences multimodaux génératifs offrent en effet la possibilité de produire des descriptions textuelles à partir d’images, lesquelles pourraient ensuite être exploitées par des scripts pour extraire les termes pertinents. Le script présenté au chapitre 3, conçu pour détecter les concepts du thésaurus Garnier dans les titres, pourrait ainsi être appliqué aux textes générés par les modèles. Toutefois, ce processus nécessite d’avoir des textes en français. Or, les modèles téléchargeables localement, tels que LLAVA ou BLIP-3, bien qu’ils évitent le recours à des API coûteuses comme celles d’OpenAI pour les modèles GPT, demeurent nettement plus performants en anglais qu’en français. 
Plusieurs pistes peuvent être envisagées pour surmonter cette barrière linguistique. Une première consiste à recourir à un système de traduction performant, tel que DeepL, afin de convertir automatiquement les textes produits en français et d’y rechercher les termes du vocabulaire contrôlé\footcite{deeplTranslateText}. Une deuxième option repose sur la traduction du thésaurus Garnier lui-même, afin de le rendre bilingue. Cette démarche pourrait être prolongée par une approche plus ambitieuse consistant à aligner les concepts du thésaurus avec ceux de Wikidata. Un tel alignement offrirait une interopérabilité accrue, en reliant chaque descripteur Garnier à un équivalent conceptuel multilingue plutôt qu’à une simple traduction littérale. Il permettrait également de croiser les données avec celles de Wikidata, notamment les peintures déjà labellisées, et d’ouvrir la voie à des rapprochements avec d’autres vocabulaires contrôlés tels qu’Iconclass, dont 4 215 concepts sont référencés dans Wikidata, ou le Getty Art and Architecture Thesaurus (AAT), dont 24 877 termes sont reliés\footnote{Consulter le chapitre 2 à ce sujet.}.
Une troisième piste réside dans le développement d’un outil de type RAG (Retrieval-Augmented Generation), reposant sur le thésaurus Garnier comme base de données, afin de traduire efficacement et de générer des résultats précis et contextualisés. Un tel modèle combine deux étapes : suite à un prompt, il interroge d’une base de données pour identifier les informations pertinentes, puis mobilise ces informations à l’aide d’un modèle génératif de langue qui produit une réponse en fonction des données des documents sources et du prompt. Dans ce cadre, un RAG pourrait analyser un texte en anglais, en identifier les correspondances pertinentes dans le thésaurus Garnier et restituer une liste des descripteurs. L’avantage de cette approche est que le modèle ne s’appuie pas uniquement sur ses paramètres internes liés à son entraînement, mais sur une base de données externe, ce qui garantit une contextualisation plus fine.
Cependant, la mise en œuvre de ces solutions se heurte à plusieurs limites. L’alignement du thésaurus Garnier avec Wikidata, bien que porteur de perspectives d’interropérabilité, est un processus extrêmement long et exigeant : le thésaurus comporte plus de 16 000 concepts, et l’alignement, pour des raisons de qualité, doit être réalisé manuellement, ce qui excède le temps imparti au projet. De même, la création d’un RAG pleinement opérationnel dans une chaîne de traitement requiert à la fois des compétences d’ingénierie numérique élevées et des ressources de calcul importantes. Lors du projet, un test mené avec un modèle Mistral 7b s’est révélé insatisfaisant, sans doute en raison d’une méthodologie inadaptée ou de la puissance limitée du modèle\footcite{mistralaiMistral7B2023}.
Pour pallier ces contraintes, nous avons eu recours à une solution alternative et gratuite proposée par Google Labs : NotebookLM\footcite{googlelabsGoogleNotebookLM}. Cet outil disponible en ligne depuis 2023 associe des documents textuels à un modèle Gemini et permet de les interroger aisément\footcite{martinGoogleNotebookLM2023}. Il s’est montré particulièrement performant pour explorer le thésaurus et identifier des équivalences. Par exemple, à partir du terme “Cléopâtre”, il parvient à proposer l’entrée précise “Cléopâtre VII” du thésaurus Garnier. Toutefois, les résultats doivent être nuancés dans de très rares cas, puisque NotebookLM tend parfois à surinterpréter. Ainsi, face à la requête “une peinture d’une femme et son enfant”, il peut parfois proposer le terme “Vierge à l’Enfant”, en se fondant sur la fréquence de ce motif iconographique. Enfin, bien que très utile pour l’exploration et l’évaluation des modèles multimodaux dans la description des images, NotebookLM ne permet pas d’industrialiser la tâche : les requêtes doivent être effectuées une par une, ce qui limite son usage dans une chaîne automatisée de traitement.

\section[Tests de performances]{Tester et comparer les performances des modèles}

Afin d’évaluer les performances d’un ensemble de modèles, ceux-ci ont été associés à un RAG développé avec NotebookLM, intégrant la liste des termes du thésaurus Garnier, et testés sur un échantillon d’images provenant du RETIF. Cette expérimentation avait pour objectif principal de mesurer l’efficacité des modèles et d’envisager la conception d’un système RAG adapté au projet, ou, le cas échéant, de privilégier d’autres méthodes pour l’extraction des termes du vocabulaire contrôlé. L’échantillon utilisé comprenait trente images issues du corpus RETIF, couvrant l’ensemble de la période étudiée, du XIIIᵉ siècle au début du XXᵉ siècle, et représentant une diversité de genres artistiques : paysages, portraits, iconographie religieuse, scènes bibliques ou mythologiques, ainsi que natures mortes\footnote{La liste des œuvres est indiquées en annexes \ref{fig:benchmark}}. Les œuvres ont été choisies de façon à représenter à la fois des œuvres facilement identifiables par les modèles, provenant de grandes collections telles que le Louvre, et des images plus difficiles à reconnaître, comme des copies, des reproductions photographiques en noir et blanc ou des compositions à l’iconographie complexe. Chaque œuvre a ensuite été indexée iconographiquement à l’aide du thésaurus Garnier de l’INHA\footnote{Nous remercions Pierre-Yves Laborde pour avoir participé à la réalisation de cette vérité terrain.}.

Quatre modèles de génération de langage naturel ont été retenus, en majorité des grands modèles de langue : GPT-4 (OpenAI)\footcite{openaiGPT4}, Mistral Medium 3\footcite{mistralaiMediumNewLarge}, LLAVA\footcite{liuVisualInstructionTuning2023} et BLIP\footcite{liBLIPBootstrappingLanguageImage2022}. À chacun d’eux a été soumis le même prompt : “Identify the scene and write an iconographical description of this painting”. Celui-ci a été formulé en anglais, puisque certains modèles, comme LLAVA et BLIP, se révèlent moins performants lorsqu’ils sont sollicités en français.

L’outil NotebookLM était utilisé pour l’extraction des termes du thésaurus. Le prompt utilisé était : “Donne une liste séparée par des virgules des termes du thésaurus présents dans cette description”, suivi de la description en anglais générée par l’un des modèles. L’outil identifiait ainsi les correspondances entre les termes anglais de la description et les descripteurs du thésaurus, fournissant un liste de termes présents dans la description. La liste obtenue a ensuite été confrontée à la vérité terrain, afin d’évaluer la pertinence et la précision de l’analyse.

\begin{table}[H]
\centering
\begin{tabular}{lcccccc}
\hline
Modèle & TP moy & FP moy & FN moy & Précision moy & Rappel moy & F1 moy \\
\hline
GPT-4 & 5.267 & 23.7 & 6.8 & 0.192 & 0.469 & 0.261 \\
Mistral M3 & 4.2 & 19.967 & 7.867 & 0.184 & 0.357 & 0.237 \\
LLAVA & 2.733 & 27.467 & 9.333 & 0.096 & 0.254 & 0.134 \\
BLIP & 0.867 & 3.2 & 11.2 & 0.229 & 0.085 & 0.116 \\
\hline
\end{tabular}
\caption{Comparaison des modèles pour l’indexation iconographique en passant par NotebookLM selon les moyennes des TP, FP, FN, précision, rappel et F1.}
\label{tab:resultats_modeles}
\end{table}

Ce tableau montre les statistiques de chaque modèle pour décrire les trente images et obtenir les termes du thésaurus Garnier. ​​Il présente une moyenne des vrais positifs, des faux positifs, des faux négatifs, de la précision, du rappel et du score F1 pour chaque modèle. Les vrais positifs correspondent au nombre de mots correctement prédits par image. Les faux positifs sont le nombre de mots prédits absents de la vérité terrain. Les faux négatifs sont les mots manquants, présents dans la vérité terrain mais pas prédits par un modèle. La précision est la proportion de mots prédits corrects parmi tous les mots prédits. Le rappel est la proportion de mots corrects corrects retrouvés parmi les mots vrais, issus de la vérité terrain. Enfin le score F1 est une moyenne harmonique entre la précision et le rappel : elle permet de donner une mesure globale de la qualité de prédiction d’un modèle. Dans l’ensemble, ces combinaisons de modèles donnent de mauvais résultats. À première vue, les scores F1 sont très bas, puisque tous inférieur à 0.7 qui serait un score F1 satisfaisant pour un modèle.

À part le modèle BLIP, l’ensemble des solutions donnent un nombre de faux positifs très importants, supérieurs à 20, soit presque le double de la moyenne de mots de la vérité terrain qui s’élève à 12,07. Cela impliquerait sûrement des corrections pour supprimer les mots supplémentaires créés par les modèles. 

\section[Automatisation et histoire de l'art]{Adapter l’automatisation aux spécificités de l’histoire de l’art}

D’autres approches ont été explorées, ne cherchant pas à mobiliser l’ensemble des termes du corpus, mais à se concentrer sur une fraction spécifique pour accomplir une tâche précise. Comme le souligne David G. Stork, les modèles d’IA traditionnels, y compris les grands modèles de langues multimodaux, ont surtout été développés pour analyser des photographies, des vidéos ou des images spécialisées, notamment les radiographies médicales\footcite[p. 72-73]{storkComputerVisionML2024}. Dans le cas des peintures, ces modèles présentent deux principales difficultés : d’une part, les caractéristiques formelles des œuvres diffèrent de la réalité photographique, et d’autre part, les datasets d’entraînement sont souvent trop restreints, particulièrement pour des iconographies rares. Ainsi, l’entraînement de modèles pour réaliser des tâches de reconnaissance ciblées pourrait permettre de développer une approche mieux adaptée aux corpus d’histoire de l’art. Nous n’avons pas identifié de modèles spécialisés capables de reconnaître efficacement des motifs ou des scènes dans les peintures de l’Époque moderne pour notre corpus. L’outil NADA (No Annotations for Object Detection in Art), développé pour reconnaître des éléments iconographiques dans des tableaux, aurait pu être pertinent, mais sa complexité l’a rendu difficile à exploiter\footcite{ramosNoAnnotationsObject2024}. Comme mentionné précédemment, la technologie de segmentation YOLO a été testée pour reconnaître des objets dans les natures mortes. Bien que cette expérience n’ait pas produit de résultats concluants, le modèle n’est pas à écarter complètement : d’autres applications restent envisageables, comme la détection de personnes ou l’extraction de la peinture de son cadre lorsque la reproduction inclut également celui-ci. 

Une autre expérience utilisant un modèle plus simple de classification a donné de meilleurs résultats. L’objectif était d’identifier les genres picturaux au sein des images du corpus, afin de distinguer les natures mortes, paysages et portraits des scènes, elles-mêmes regroupant les scènes religieuses, de genre et mythologiques. Un corpus d’entraînement de 1 495 peintures du RETIF a été constitué, réparti en quatre classes : 258 natures mortes, 227 paysages, 384 portraits et 626 scènes. L’outil utilisé est la bibliothèque Python FastAI, construite sur PyTorch, conçue pour faciliter le développement et la manipulation de modèles d’apprentissage profond\footcite{DocumentationFastAI}. La partie FastAI Vision de cette bibliothèque est dédiée au traitement d’images, notamment la classification, et sa simplicité d’utilisation permet des expérimentations rapides. Le modèle s’appuie sur le réseau de neurones convolutif ResNet, pré-entraîné sur le dataset ImageNet de 1,2 million d’images, ce qui lui permet de détecter des motifs visuels génériques tels que les contours, textures et formes\footcite{heDeepResidualLearning2015}. Lors du réentraînement du modèle de classification, des transformations sont appliquées automatiquement aux images (rotation, recadrage, inversion miroir, variation de luminosité) afin d’améliorer sa robustesse et la reconnaissance des classes qu’il doit identifier. Après seulement dix époques d’entraînement, le modèle a été évalué sur le corpus de validation, donnant des résultats très encourageants : 

\begin{table}[H]
\centering
\begin{tabular}{c c c c}
\hline
\textbf{Exactitude} & \textbf{Précision} & \textbf{Rappel} & \textbf{F1-score} \\
\hline
0.8428 & 0.8386 & 0.8432 & 0.8336 \\
\hline
\end{tabular}
\caption{Performances du modèle de classification des genres picturaux}
\label{tab:classification_genres_horizontal}
\end{table}

Ces statistiques montrent que le modèle est performant pour la reconnaissance des genres picturaux, avec un taux d’erreur relativement faible (Matrice de confusion : \ref{fig:matrice}). Ce modèle peut ainsi constituer une première étape prometteuse pour l’indexation iconographique des œuvres.
D’autres modèles avaient également été envisagés, notamment FLAVA (A Foundational Language And Vision Alignment Model), conçu pour établir des correspondances plus fines entre les images et leurs catégories de représentation. Développé par Facebook AI Research (FAIR) et présenté en 2021, ce modèle a été entraîné sur près de 70 millions de paires texte-image\footcite{singhFLAVAFoundationalLanguage2021}. Il est notamment capable de prédire si une image et un texte correspondent en mobilisant trois encodeurs distincts, chacun dédié respectivement aux tâches de vision par ordinateur, de traitement du langage et aux tâches multimodales. Son utilisation avait été envisagée dans le cadre de notre étude pour exploiter les titres des images, en particulier pour les scènes à iconographie récurrente, telles que les scènes bibliques, les épisodes de l’Ancien Testament, de la vie de la Vierge ou de celles du Christ. La manipulation du modèle n’a pas pu être expérimentée, faute de temps et de compétences techniques.

Ces expériences, bien que parfois peu concluantes, ouvrent la voie à une première ébauche de protocole méthodologique adapté au corpus pour réaliser l’automatisation de son indexation iconographique. Une phase initiale pourrait consister à déterminer les genres iconographiques à l’aide du modèle de classification des images, avant d’affiner l’analyse par des outils plus performants en fonction de la complexité des représentations. Ainsi, les scènes, plus complexes à interpréter, pourraient faire l’objet d’un traitement spécifique, différent de celui appliqué aux natures mortes, où l’enjeu principal se limite à l’identification des objets. L’utilisation des grands modèles de langue pourrait alors être réservée aux cas les plus complexes, avec extraction des termes pertinents du thésaurus soit via un outil de recherche automatique dans le texte généré, soit au moyen d’une approche de génération augmentée par récupération (RAG).

