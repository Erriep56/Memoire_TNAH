L’indexation iconographique est une approche qui repose majoritairement sur le texte, en décrivant l’image à l’aide de concepts exprimés en langage naturel, ce qui peut en limiter la portée. L’intelligence artificielle ouvre de nouvelles perspectives sur cette pratique documentaire en établissant un lien multimodal entre texte et image. Elle permet également d’envisager des méthodes de recherche qui dépassent le langage naturel, en mobilisant directement des motifs visuels ou des images comme points d’accès à l’information.


\section[Multimodalité]{La multimodalité texte-image offerte par l'intelligence artificielle}

En intelligence artificielle, la multimodalité désigne la capacité d’un modèle à combiner et à raisonner à partir de types de données hétérogènes, tels que des textes, des images, des fichiers audio ou vidéo. En 2023, avec l’usage croissant de l’outil CLIP dans les projets liés aux images, Thomas Smits et Melvin Meyers ont évoqué « un tournant multimodal dans les humanités numériques »\footcite{smitsMultimodalTurnDigital2023}. CLIP (Contrastive Language-Image Pre-training) est une technologie conçue pour entraîner des modèles à associer textes et images\footcite{radfordLearningTransferableVisual2021}. La version originale de ce modèle a été développée par la société américaine OpenAI et publiée en 2021. Elle a été entraînée sur près de 400 millions de paires image-texte, c’est-à-dire d’images accompagnées de leurs légendes, afin de créer un espace de représentation commun dans lequel les images se rapprochent des descriptions textuelles qui leur correspondent et s’éloignent de celles qui ne correspondent pas. Pour atteindre cet objectif, CLIP repose sur deux encodeurs distincts. Le premier, basé sur des transformeurs, encode un texte en un vecteur reflétant son contenu sémantique. Le second, fondé sur des réseaux de vision, encode une image en un vecteur décrivant son contenu visuel. Chaque description textuelle et chaque image sont ainsi projetées dans un même espace de représentation vectorielle, ce qui permet de comparer directement le texte et l’image. Le modèle peut ainsi identifier les textes qui décrivent le mieux une image, ou les images correspondant à un mot ou une requête en langage naturel, sans nécessiter de nouvel entraînement spécifique pour chaque tâche. Par exemple, pour la requête « un chien », CLIP encode le texte en vecteur et le compare aux vecteurs correspondant à un ensemble d’images. Les images dont la représentation est la plus proche de celle du texte sont considérées comme représentant cet animal. Cette technologie permet donc, entre autres, de rechercher dans un corpus d’images des mots ou des phrases en langage naturel, même si ces images n’ont pas été préalablement annotées ou indexées. Les modèles entraînés à l’aide de cette technologie sont relativement légers et ne nécessitent pas de configuration informatique ou de processeurs graphiques puissants. Une fois téléchargé, il peut être utilisé localement pour encoder des images et effectuer des recherches dans un corpus, sans connexion à Internet.

Plusieurs projets à travers le monde ont mis en place des moteurs de recherche basés sur des modèles CLIP afin d’améliorer la découvrabilité de collections patrimoniales. Cette technologie est notamment utilisée pour explorer des ensembles de données variés, comme les collections de photographies japonaises du Musée Guimet dans le cadre du projet HikarIA\footcite{HikarIA}, ou les collections de cartes postales de la Bibliothèque nationale du Danemark\footcite{haffendenUnearthingForgottenImages2023}. La Bibliothèque nationale de France, dans le cadre de son projet Gallica Images, intègre CLIP en fin de chaîne de traitement. Soutenu par le programme France 2030 et réalisé en coopération avec l’INHA et la Bibliothèque nationale universitaire de Strasbourg, ce projet vise à rendre accessibles les fonds iconographiques numérisés dans Gallica, la bibliothèque numérique de la BnF et de ses partenaires\footcite{bibliothequenationaledefranceProjetsIntelligenceArtificielle}. Grâce aux outils d’intelligence artificielle, les images sont extraites de recueils, albums ou documents à l’aide d’algorithmes de segmentation. Un moteur de recherche intégrant CLIP facilite ensuite l’exploration de ces images, permettant de naviguer efficacement dans le vaste corpus d’images patrimoniales proposées sur Gallica\footnote{\cite{adamovaProjetGallicaImages2024}. L’outil a été expérimenté dans les locaux de l’INHA entre avril et juillet 2025.}.

Dans le cadre du projet RETIF, nous avons développé une application web intégrant un moteur de recherche CLIP à l’aide de la bibliothèque open-source Python Streamlit, qui permet de créer rapidement une interface de présentation (annexe \ref{code-streamlit})\footnote{streamlitDocumentation}. Cette application reproduit les fonctionnalités offertes par l’outil Panoptic : elle récupère les encodages des images préalablement préparés et, grâce à un moteur de recherche textuel basé sur CLIP, propose des images proches correspondant à l’idée exprimée. Le modèle expérimenté est ViT-B/32\footnote{Le modèle Vit-L, plus puissant, demandait trop de ressources pour l’encodage des images par rapport à celles disponibles.}. Cette approche dépasse les limites de l’indexation iconographique traditionnelle, en permettant de détecter des concepts et des caractéristiques souvent négligés dans les pratiques documentaires classiques : la présence de couleurs, l’intensité lumineuse, la récurrence de motifs communs comme le ciel, les nuages ou l’herbe, ainsi que des dimensions affectives générales de la composition, telles que la tristesse ou la joie. Si les requêtes simples fournissent généralement des résultats pertinents et conformes à la demande (figures \ref{fig:APP-pyramid} et \ref{fig:APP-monalisa}), les modèles CLIP présentent encore de nombreuses limites pour analyser et comprendre les peintures, notamment en ce qui concerne les styles picturaux ou les iconographies complexes\footcite{aspertiDoesCLIPPerceive2025}. Par ailleurs, le contenu exact de WebImageText (WIT), le jeu de données interne développé par OpenAI, n’étant pas publié pour des raisons de droits d’auteur et de filtrage, il reste impossible d’évaluer la proportion d’œuvres d’art picturales qu’il contient\footcite[p. 4-5]{radfordLearningTransferableVisual2021}.

\section[Amélioration de CLIP]{Améliorer un modèle de CLIP pour répondre aux problématiques de la peinture ?}

Ainsi, un test a été mené pour affiner un modèle CLIP préexistant afin d’améliorer la reconnaissance des motifs et des scènes dans les images du RETIF, en particulier avec des termes en français. Le jeu d’entraînement était constitué des 11 966 images du corpus accompagnées d’une courte légende mentionnant le nom de l’artiste, le titre de l’œuvre et les termes d’indexation associés, identifiés lors du travail sur l’application Panoptic. Après le fine-tuning et l’intégration du modèle dans l’application Streamlit développée précédemment, les résultats se sont révélés très décevants et nettement inférieurs à ceux du modèle VIT-B/32, utilisé précédemment. Le système ne présentait plus aucune cohérence, et même les recherches liées à des termes bien représentés et indexés dans le corpus (par exemple « nature morte », « Vierge », « chien ») aboutissaient à des résultats aberrants. Nos compétences techniques limitées et le temps restreint alloué à l’expérience ne nous ont pas permis d’identifier précisément l’origine du problème. Toutefois, plusieurs hypothèses peuvent être formulées. Nos 12 000 paires images-textes constituaient peut-être un volume insuffisant pour atteindre un niveau de performance comparable à celui des modèles entraînés sur de plus grandes quantités de données. D’autres raisons techniques peuvent expliquer l’inefficacité de l'affinement de notre modèle : un entraînement limité à deux époques, l’utilisation d’un système d’exploitation Windows au lieu de Linux, qui est préconisé, et des ressources de calcul insuffisantes. Sur la machine de l’INHA, dotée d’une carte graphique NVIDIA GeForce RTX 4080, l’opération avait déjà mobilisé plus de quatre heures.

Un autre facteur de difficulté tient à la complexité visuelle des images du RETIF. Là où les jeux de données classiques présentent souvent une image contenant un seul objet accompagné d’une description simple (« un chien », « une pomme »), nos peintures réunissent fréquemment de nombreux éléments simultanés (une nature morte composée d’un globe terrestre, de cartes marines, d’instruments mathématiques sous une draperie de tissu, par exemple). L’absence d’annotations précises sur la localisation et la hiérarchie de ces objets peut limiter l’efficacité de l’apprentissage.

Une stratégie plus pertinente pour améliorer CLIP dans le contexte pictural aurait pu consister soit à constituer un corpus de motifs iconographiques isolés (par exemple, uniquement la figuration d’un chien ou d’un fruit dans une peinture) assortis de descriptions simples, soit à adapter des jeux de données photographiques existants pour leur donner une apparence picturale. Cette seconde approche, de transformer des photographies en images simulant les effets de texture de la peinture à l’huile, médium le plus présent dans le corpus du RETIF, est d’ailleurs recommandée par Thomas Heitzinger David G. Stork dans une étude où ils rapportent l’efficacité de la méthode pour des modèles de segmentation\footcite{heitzingerImprovingSemanticSegmentation2022}. Cette démarche permettrait au modèle de se concentrer moins sur la catégorisation des images du RETIF en tant que « peintures » et davantage sur l’identification des motifs iconographiques qu’elles représentent, indépendamment de leur caractère pictural.

\section{Une approche image-à-image}

En 2016, Erik Malcolm Champion mettait en évidence la prépondérance des études textuelles au sein des humanités numériques et plaidait pour une plus grande autonomie des images et des médias par rapport au texte\footcite{championDigitalHumanitiesText2016}.  L’année suivante, Chiara Franceschini appelait à repenser les méthodologies de l’iconographie, trop dépendantes des descriptions textuelles, et évoquait le « tournant visuel » en histoire de l’art rendu possible par les nouvelles technologies\footnote{\cite[p. 190]{franceschiniClassifyingImageContent2017} : « The classic methodology of iconography, with its strong tendency to anchor image to text, if not to a source, and to a subject matter (described either with a text or a simple textual label), needs to be rethought. [...] Without sounding too paradoxical, it may be possible to say that we are today assisting in a visual turn in art history»}. En effet, au sein des méthodes de cette discipline, la comparaison d’images est particulièrement pertinente pour identifier des copies, des variations ou des transferts de motifs entre œuvres, pour comparer différentes versions d’un même thème ou pour étudier les compositions dans le cadre de séries ou de multiples. Pour répondre à des problématiques nécessitant la recherche d’images comparatives à partir d’une image donnée, des outils numériques ont été développés afin de permettre une recherche visuelle en se détachant des contraintes textuelles.

C’est notamment le cas de la plateforme Ukiyo-e.org, créée en 2012 par l’ingénieur John Resig\footcite{resigUkiyoeorgJapanesePrint2012}. Elle contient plus de 200 000 estampes japonaises provenant de plus de 24 institutions dans le monde et propose deux méthodes de recherche : la première repose sur un moteur de recherche textuel classique exploitant les métadonnées des œuvres, tandis que la seconde permet de rechercher dans la base des images similaires à une photographie d’estampe téléversée par l’utilisateur\footcite{resigSite2012}. Les estampes issues des différentes collections sont regroupées par similarité pour mettre en valeur les différents états et impressions d’une même estampe répartis dans les diverses institutions.

Le projet Replica poursuivait un objectif similaire de restauration de l’autonomie des images et de facilitation des liens entre elles pour la recherche. Développé par Benoît Seguin à l’École Polytechnique Fédérale de Lausanne sous la supervision de Frédéric Kaplan, et en partenariat avec la Fondation Cini à Venise, les premiers résultats ont été publiés en 2016\footcite{seguinVisualLinkRetrieval2016}. Le principe était de créer un moteur iconographique basé sur l’image, utilisant des réseaux de neurones convolutifs (CNN) pour identifier des motifs récurrents et des similarités compositionnelles entre les œuvres. Ces recherches s’appuyaient sur les grandes théories de l’histoire de l’art, notamment celles d’Aby Warburg et d’Ernst Gombrich, et permettaient d’exploiter l’immense fonds photographique de l’institution vénitienne. Xi Shen, Alexei A. Efros et Mathieu Aubry ont également exploré à plusieurs reprise la similarité de motifs dans les arts visuels, en particulier entre différents médiums comme les peintures et les dessins\footcites{shenDiscoveringVisualPatterns2019}{shenSpatiallyConsistentFeatureMatching2022}.

Avec l’arrivée de CLIP et la mise à disposition de modèles entraînés avec cette technologie, de nouvelles perspectives se sont ouvertes pour la recherche par similarité d’images, ajoutant une dimension sémantique aux comparaisons visuelles. L’encodage et la création de vecteurs d’embeddings permettent de mesurer la proximité entre images selon leur contenu tant visuel que sémantique. Ainsi, de nombreux moteurs de recherche intègrent désormais cette fonctionnalité, permettant de trouver des images similaires non pas à partir de textes, mais directement à partir d’une image.

Par exemple, ICONCLASS propose à ses membres un outil multimodal permettant d’explorer un jeu de données entièrement indexé selon son système iconographique. L’outil facilite l’indexation en montrant des images similaires à une représentation que l’on souhaite décrire\footnote{Je tiens à remercier Hans Brandhorst et Etienne Posthumus de m’avoir permis de tester l’outil en mars 2025. \cite{santiniMultimodalSearchIconclass2023} ; \cite[diapositive 14 de la présentation]{brandhorstICONCLASSplus}}. 

Dans le cadre du projet RETIF, la recherche inversée à l’aide d’une image a été testée dans l’application développée avec Streamlit et CLIP que nous évoquions précédemment. Cette approche permet d’identifier non seulement des copies d’une même œuvre, mais aussi certains motifs iconographiques. Le modèle encode l’image ajoutée par l’utilisateur et présente les images similaires de la plus proche à la moins proche. En exploitant son apprentissage à partir de paires image‑texte, il est capable d’identifier des représentations différentes d’un point de vue formel mais similaires en termes de contenu. Ainsi, une photographie d’un poisson peut permettre d’extraire les nature mortes du RETIF où cet animal est figuré (figure \ref{fig:APP-poisson}). Selon Polo Sologub et Rebecca Fiebrink, CLIP offre même d’excellents résultats lorsqu’on lui soumet un croquis d’un objet à retrouver\footcite{sologubSketchyCollectionsExploring2024}. Ce type de recherche par l’image constitue un atout majeur pour les historiens de l’art et les utilisateurs de bases de données, en particulier pour identifier des éléments ou objets qui n’ont pas été décrits lors de l’indexation.

\bigskip

Ainsi, les outils d’intelligence artificielle fondés sur la similarité des images et la multimodalité, tels que CLIP, ouvrent de nouvelles perspectives pour l’indexation iconographique et proposent des approches complémentaires pour l’exploration des corpus visuels.