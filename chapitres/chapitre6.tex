L’automatisation constitue donc un véritable défi, car elle exige d’adapter les pratiques de l’histoire de l’art aux outils numériques. Si elle offre des perspectives prometteuses pour le traitement de vastes corpus, elle requiert néanmoins des ressources considérables, tant humaines que matérielles, à chaque étape du processus.

\section[Bénéfices]{Des bénéfices non négligeables : traitement de masse, reproductibilité et transférabilité des méthodes}

L’automatisation offre des avantages qui justifient son recours dans le cadre des collections patrimoniales. Elle permet avant tout de répondre à la contrainte de temps considérable nécessaire à l’étude et à l’inventaire des fonds. À titre d’exemple, dans le cadre des projets Royère et Torne-H, qui mobilisent des outils de vision par ordinateur pour analyser les collections du Musée des Arts Décoratifs, la réalisation manuelle de l’inventaire et de la description des près de 700 000 œuvres non encore répertoriées de la collection de dessins, papiers peints et photographies a été estimée à 643 ans\footcite{bermesRepenserCollectionsPatrimoniales2025}. L’intelligence artificielle apparaît ainsi comme une solution pour accélérer le travail des conservateurs et documentalistes. Elle contribuerait à rendre les collections plus accessibles et à faciliter la consultation et l’exploration des bases de données. Un autre bénéfice réside dans la transférabilité des méthodes développées. Les techniques d’automatisation mises en œuvre dans un projet donné peuvent être adaptées à d’autres contextes institutionnels. C’est le cas du projet Torne-H, mais également du projet HikarIA, mené conjointement par le Musée national des Arts Asiatiques – Guimet et la société Teklia. L’objectif est d’étendre le traitement au-delà de la collection Dubois, afin d’inclure l’ensemble des albums de photographies japonaises anciennes conservées au musée, et avec la perspective de transposer la démarche à d’autres collections comparables dans les institutions patrimoniales au Japon\footcite{kermorvantProjetHikarIAAnalyse2025}.

Dans le cadre du projet de l’INHA consacré à l’indexation iconographique du RETIF, le recours à des méthodes automatisées offrirait la possibilité de transférer ce processus vers d’autres bases de données de peintures d’AGORHA insuffisamment indexées. Ainsi, la méthodologie développée dans le cadre de ce projet pourrait être appliquée à des corpus tels que l’Iconographie musicale : répertoire d'œuvres d'art à sujets musicaux publiées par Albert Pomme de Mirimonde\footcite{institutnationaldhistoiredelartIconographieMusicaleRepertoire}, le Répertoire des tableaux français en Allemagne (XVII\textsuperscript{e} et XVIII\textsuperscript{e} siècles) (REPFALL)\footcite{institutnationaldhistoiredelartRepertoireTableauxFrancais}, le Recensement de la peinture produite en France au XVI\textsuperscript{e} siècle\footcite{institutnationaldhistoiredelartRecensementPeintureProduite}, ou encore Les Envois de Rome en peinture et sculpture, 1804-1914\footcite{institutnationaldhistoiredelartEnvoisRomePeinture}. L’automatisation offre ainsi les perspectives d’économie de temps et de transférabilité des méthodes, dont les apports seraient significatifs pour l’étude des collections patrimoniales.

\section[Des limites]{Des limites institutionnelles et techniques}
Cependant, l’automatisation d’un tel processus soulève des enjeux importants, tant institutionnels, organisationnels que infrastructurels, auxquels le projet de l’INHA a dû faire face. Contrairement à d’autres projets similaires, une équipe pluridisciplinaire dédiée n’avait pas été constituée pour travailler exclusivement sur ce projet. L’ensemble du travail reposait principalement sur une seule personne, soutenue ponctuellement par les ingénieurs du Service numérique de la recherche de l’INHA et par des compétences techniques issues de PictorIA. Un croisement plus important des compétences, combinant une expertise iconographique approfondie du corpus étudié et des savoir-faire en vision par ordinateur, aurait été nécessaire, tant pour la préparation du projet que pour le déroulement des expérimentations. Si quatre mois ont permis d’obtenir des premiers résultats, les expériences d’autres projets français ont révélé la nécessité de mettre en œuvre des périodes de préparation, de test et de manipulation des modèles beaucoup plus longues pour atteindre des résultats probants. À titre d’exemple, après un an d’expérimentations avec un modèle YOLO pour étudier le fonds de dessins du décorateur Jean Royère (1902-1981), Emmanuelle Bermès et Marion Charpier notent que « le degré de précision que nous recherchons pour répondre aux attentes du personnel du musée n’est pas encore atteint »\footcite{bermesRepenserCollectionsPatrimoniales2025}. Un autre facteur limitant concerne les infrastructures informatiques, en grande partie liées à la puissance de calcul requise pour entraîner et déployer des modèles de vision par ordinateur. Les processeurs graphiques, ou GPU (pour \textit{Graphical processor unit}) initialement conçus pour le traitement massif d’images et de graphiques dans les ordinateurs et consoles de jeu, sont aujourd’hui indispensables pour l’intelligence artificielle, en particulier pour les réseaux de neurones profonds, qui impliquent des calculs intensifs de matrices et vecteurs. Ces architectures permettent des traitements parallèles massifs, nécessaires pour entraîner des modèles complexes. De plus en plus d’institutions menant des projets en sciences humaines se dotent de processeurs graphiques pour leurs travaux de recherche. Par exemple, l’École des Chartes et le MediaLab de SciencesPo\footnote{Ces informations m’ont été communiquées par Mathieu Taybi et Lilla Conte.}, ainsi que l’Université Paris-Cité\footcite{universitepariscitePlateformeCalculHautePerformance}, disposent de ces ressources, tout comme l’infrastructure de recherche Huma-Num, qui met des infrastructures à disposition\footcite{huma-numInfrastructureMulticiblesPour2025}. Ces équipements restent cependant coûteux, tant pour leur acquisition que pour leur consommation électrique. À l’Institut national d’histoire de l’art, à l’exception d’un ordinateur équipé d’une carte graphique Nvidia 4080 au sein du Service numérique de la recherche, aucune ressource comparable n’est disponible pour l’intelligence artificielle. Aucun projet de l’institution n’intègre actuellement de telles technologies, à l’exception du projet Gallica Images, mené en collaboration avec la Bibliothèque nationale de France et la Bibliothèque nationale universitaire de Strasbourg\footcite{bibliothequenationaledefranceProjetsIntelligenceArtificielle}. Pour les expérimentations réalisées dans le cadre du projet d’indexation du RETIF, l’ordinateur disponible et les GPU gratuits proposés par Google Colab ont suffi, mais ils ne permettraient pas une mise à grande échelle d’un traitement automatisé\footcite{googleBienvenueDansColab}. Pour des projets futurs nécessitant des infrastructures plus conséquentes, il apparaît donc préférable de privilégier des collaborations avec des institutions déjà équipées, plutôt que d’acquérir du matériel qui serait peu utilisé.

\section{L'impact environnemental}

Depuis le déploiement des grands modèles de langage auprès du grand public, leur impact environnemental a rapidement été souligné. Dans ce contexte, des outils sont apparus, tels que Compar:IA, conçu pour mesurer l’empreinte écologique des requêtes adressées aux agents conversationnels\footcite{ministeredelacultureComparIAComparateur2024}. Parallèlement, les premières études ont commencé à être publiées afin de mieux documenter ces effets. Le 22 juillet 2025, la société française Mistral AI publiait une étude intitulée textit{Our contribution to a global environmental standard for AI}, menée en collaboration avec le cabinet de conseil Carbone 4 et l’Agence de l'environnement et de la maîtrise de l'énergie (ADEME)\footcite{mistralaiOurContributionGlobal2025}. Cette recherche visait à quantifier l’empreinte environnementale des grands modèles de langage (LLM) selon trois critères principaux : les émissions de gaz à effet de serre, la consommation d’eau et l’épuisement des ressources non renouvelables. L’analyse s’appuie sur le suivi du modèle Mistral Large 2, utilisé pendant 18 mois jusqu’en janvier 2025. Les résultats font état de 20,4 kilotonnes de CO₂e consommés pour l'entraînement du modèle. Rapporté aux émissions annuelles moyennes par habitant en France en 2023 (9,4 tonnes), cela équivaut aux émissions de plus de 2 000 Français en un an\footcite{inseeEmissionsGazEffet2025}. Pour la consommation d’eau, l’étude chiffre à volume utilisé de 281 000 m³, soit l’équivalent de la consommation domestique annuelle de plus de 5 200 habitants (sur la base d’une moyenne de 54 m³ par personne en 2020)\footcite{commissariatgeneralaudeveloppementdurableConsommationDomestiqueEau2023}. L’essentiel de cette empreinte provient de la phase d’entraînement des modèles, responsable de 85,5 \% des émissions de CO₂ et de 91 \% de la consommation d’eau potable. À l’inverse, l’impact d’une requête d’inférence reste marginal : environ 1,14 g de CO₂e et 45 mL d’eau par demande. Toutefois, l’accumulation de requêtes à des grands modèles de langue, par exemple appliquées aux 11 974 images du RETIF, peut représenter une dépense énergétique notable, d’autant plus lorsque les résultats peuvent nécessiter une curation ou une analyse complémentaire. L’impact environnemental de ces outils est donc non négligeable est doit être pris en compte lors de l’utilisation de ces outils.

\bigskip

Si l’automatisation du processus peut être envisagée pour certaines tâches de reconnaissance, les tests menés ont mis en évidence de nombreuses difficultés, mobilisant un temps conséquent d’expérimentation ainsi que d’importants moyens techniques et humains afin d’établir un protocole. Si ces expériences ont permis de formuler des hypothèses pour l’analyse d’images intégrant en intégrant un vocabulaire contrôlé, d’autres approches peuvent être envisagées à l’automatisation afin de tirer pleinement parti des systèmes de vision par ordinateur dans l’indexation iconographique.